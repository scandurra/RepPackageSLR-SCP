"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Trust-Based Cloud Machine Learning Model Selection for Industrial IoT and Smart City Services","B. Qolomany; I. Mohammed; A. Al-Fuqaha; M. Guizani; J. Qadir","Department of Cyber Systems, College of Business and Technology, University of Nebraska at Kearney, Kearney, NE, USA; Department of Computer Science, Western Michigan University, Kalamazoo, MI, USA; Information and Computing Technology Division, College of Science and Engineering, Hamad Bin Khalifa University, Doha, Qatar; Computer Science and Engineering Department, Qatar University, Doha, Qatar; Department of Electrical Engineering, Information Technology University, Lahore, Pakistan","IEEE Internet of Things Journal","5 Feb 2021","2021","8","4","2943","2958","With machine learning (ML) services now used in a number of mission-critical human-facing domains, ensuring the integrity and trustworthiness of ML models becomes all important. In this work, we consider the paradigm where cloud service providers collect big data from resource-constrained devices for building ML-based prediction models that are then sent back to be run locally on the intermittently connected resource-constrained devices. Our proposed solution comprises an intelligent polynomial-time heuristic that maximizes the level of trust of ML models by selecting and switching between a subset of the ML models from a superset of models in order to maximize the trustworthiness while respecting the given reconfiguration budget/rate and reducing the cloud communication overhead. We evaluate the performance of our proposed heuristic using two case studies. First, we consider Industrial IoT (IIoT) services, and as a proxy for this setting, we use the turbofan engine degradation simulation data set to predict the remaining useful life of an engine. Our results in this setting show that the trust level of the selected models is 0.49%-3.17% less compared to the results obtained using integer linear programming (ILP). Second, we consider smart cities services, and as a proxy of this setting, we use an experimental transportation data set to predict the number of cars. Our results show that the selected model's trust level is 0.7%-2.53% less compared to the results obtained using ILP. We also show that our proposed heuristic achieves an optimal competitive ratio in a polynomial-time approximation scheme for the problem.","2327-4662","","10.1109/JIOT.2020.3022323","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9187421","Adversarial attacks;automatic model selection;deep learning (DL);Industrial IoT (IIoT);ML as a Service (MLaaS);smart city;trusted machine learning (ML) models","Data models;Cloud computing;Predictive models;Machine learning;Internet of Things;Smart cities;Predictive maintenance","cloud computing;computational complexity;integer programming;Internet of Things;jet engines;learning (artificial intelligence);linear programming;production engineering computing;smart cities;trusted computing","ML-based prediction models;cloud service providers;machine learning services;smart city services;trust-based cloud machine learning model selection;smart cities services;trust level;turbofan engine degradation simulation data;Industrial IoT services;cloud communication overhead;trustworthiness;intermittently connected resource-constrained devices","","24","","70","IEEE","7 Sep 2020","","","IEEE","IEEE Journals"
"AACF—Accessible Application-Centric Framework for the Internet of Things Backhauled Smart City Applications","Y. Cui; X. Song; J. Liu; K. Chen; G. Shi; J. Zhou; T. GS","Beihang University, Beijing, China; Beihang University, Beijing, China; School of Electronic and Information Engineering, University of Aeronautics & Astronautics, Beijing, China; Beihang University, Beijing, China; Beijing Institute of Electronic System Engineering and Simulation Center, Beijing, China; Beijing Institute of Electronic System Engineering and Simulation Center, Beijing, China; School of Information Technology and engineering, VIT University, Vellore, India","IEEE Transactions on Network Science and Engineering","24 May 2022","2022","9","3","980","989","The existing Internet of Things (IoT) architectures are employed for different real-time smart city applications adaptively. The architecture's adaptability to the application depends on the robust quality of experience (QoE) granted to the end-users. This article introduces an Accessible Application-Centric Framework (AACF) for application-specific IoT architectures by considering the robust requirements of end-user applications. The framework aims to provide flexible application-service disseminations for end-user requests. The state of the application and its longevity are taken into account for satisfying the in-time need of the requests. The service requiring scalable dissemination is performed based on state learning decisions to improve the success rate and enduring responses. In this learning process, the balance between response and service dissemination is retained to pursue robust user-specific performance. The proposed AACF is assessed using the metrics response ratio, response delay, and backlogs.","2327-4697","","10.1109/TNSE.2021.3099579","National Key Research and Development Program of China(grant numbers:2018YFB1702703); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9496692","Adaptability;IoT;machine learning;service applications;social computing","Smart cities;Internet of Things;Quality of service;Quality of experience;Resource management;Delays;Decision making","Internet of Things;mobile computing;smart cities","QoE;quality of experience;Accessible Application-Centric Framework;AACF;real-time smart city applications;Internet of Things;robust user-specific performance;state learning decisions;end-user requests;flexible application-service disseminations;robust requirements;application-specific IoT architectures;robust quality","","","","30","IEEE","26 Jul 2021","","","IEEE","IEEE Journals"
"A Hybrid Deep Learning Approach for Replay and DDoS Attack Detection in a Smart City","A. A. Elsaeidy; A. Jamalipour; K. S. Munasinghe","Faculty of Science and Technology, University of Canberra, Bruce, ACT, Australia; School of Electrical and Information Engineering, The University of Sydney, Camperdown, NSW, Australia; Faculty of Science and Technology, University of Canberra, Bruce, ACT, Australia","IEEE Access","24 Nov 2021","2021","9","","154864","154875","Today’s smart city infrastructure is predominantly dependant on Internet of Things (IoT) technologies. IoT technology essentially facilitates a platform for service automation through connections of heterogeneous objects via the Internet backbone. However, the security issues associated with IoT networks make smart city infrastructure vulnerable to cyber-attacks. For example, Distributed Denial of Service (DDoS) attack violates the authorization conditions in smart city infrastructure; whereas replay attack violates the authentication conditions in smart city infrastructure. Both attacks lead to physical disruption to smart city infrastructure, which may even lead to financial loss and/or loss of human lives. In this paper, a hybrid deep learning model is developed for detecting replay and DDoS attacks in a real life smart city platform. The performance of the proposed hybrid model is evaluated using real life smart city datasets (environmental, smart river and smart soil), where DDoS and replay attacks were simulated. The proposed model reported high accuracy rates: 98.37% for the environmental dataset, 98.13% for the smart river dataset, and 99.51% for the smart soil dataset. The results demonstrated an improved performance of the proposed model over other machine learning and deep learning models from the literature.","2169-3536","","10.1109/ACCESS.2021.3128701","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9617591","Intrusion detection;distributed denial of service (DDoS) attacks;replay attack;smart city;deep learning;Internet of Things (IoT)","Smart cities;Deep learning;Denial-of-service attack;Computer crime;Convolutional neural networks;Intrusion detection;Internet of Things","authorisation;computer network security;deep learning (artificial intelligence);Internet;Internet of Things;smart cities","hybrid deep learning;DDoS attack detection;smart city infrastructure;distributed denial of service attack;authorization conditions;DDoS attacks;real life smart city platform;smart river;smart soil;machine learning;Internet of Things;IoT technologies;Internet backbone;authentication","","9","","57","CCBYNCND","16 Nov 2021","","","IEEE","IEEE Journals"
"Machine learning-based radio access technology selection in the Internet of moving things","R. Sanchez-Iborra; L. Bernal-Escobedo; J. Santa","Dept. Engineering and Applied Techniques, University Centre of Defence at the Spanish Air Force Academy, San Javier, Spain; Dept. Information and Communication Engineering, University of Murcia, Murcia, Spain; Dept. of Electronics, Computing Technology and Projects, Technical University of Cartagena, Cartagena, Spain","China Communications","26 Jul 2021","2021","18","7","13","24","The Internet of Moving Things (IoMT) takes a step further with respect to traditional static IoT deployments. In this line, the integration of new eco-friendly mobility devices such as scooters or bicycles within the Cooperative-Intelligent Transportation Systems (C-ITS) and smart city ecosystems is crucial to provide novel services. To this end, a range of communication technologies is available, such as cellular, vehicular WiFi or Low-Power Wide-Area Network (LPWAN); however, none of them can fully cover energy consumption and Quality of Service (QoS) requirements. Thus, we propose a Decision Support System (DSS), based on supervised Machine Learning (ML) classification, for selecting the most adequate transmission interface to send a certain message in a multi-Radio Access Technology (RAT) set up. Different ML algorithms have been explored taking into account computing and energy constraints of IoMT enddevices and traffic type. Besides, a real implementation of a decision tree-based DSS for micro-controller units is presented and evaluated. The attained results demonstrate the validity of the proposal, saving energy in communication tasks as well as satisfying QoS requirements of certain urgent messages. The footprint of the real implementation on an Arduino Uno is 444 bytes and it can be executed in around 50 µs.","1673-5447","","10.23919/JCC.2021.07.002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9495351","internet of moving things;multi-RAT;CITS;classification;personal mobility","Radio access technologies;Quality of service;Wireless fidelity;TCPIP;Radio access technologies;Decision support systems;Monitoring","decision support systems;decision trees;Internet of Things;learning (artificial intelligence);mobile computing;quality of service;radio access networks;telecommunication network topology;telecommunication traffic","satisfying QoS requirements;decision tree-based DSS;IoMT enddevices;different ML algorithms;multiRadio Access Technology;adequate transmission interface;Decision Support System;Service requirements;energy consumption;communication technologies;smart city ecosystems;Cooperative-Intelligent Transportation Systems;bicycles;scooters;eco-friendly mobility devices;traditional static IoT deployments;Machine learning-based radio access technology selection;memory size 444.0 Byte;time 50.0 mus","","5","","","","26 Jul 2021","","","IEEE","IEEE Magazines"
"A Survey on IoT Big Data Analytic Systems: Current and Future","Y. Sasaki","Graduate School of Information Science Technology, Osaka University, Suita, Japan","IEEE Internet of Things Journal","6 Jan 2022","2022","9","2","1024","1036","The Internet of Things (IoT) has become widespread around the world. Since a large number of diverse devices, such as vehicles, household electrical appliances, smart phones, and environmental sensors are connected to the Internet, we can obtain a large volume of diverse IoT data, known as IoT big data. The generation of IoT big data means that efficient analytic systems are needed for many application scenarios, for example, to optimize urban planning, solve air pollution problems, and improve business decisions. In this survey, we review current systems that can efficiently analyze IoT data. Existing systems can be categorized into batch and stream processing systems. We explore Hadoop- and Spark-based batch processing systems for spatiotemporal and trajectory data. We also review fog- and edge-aware stream processing systems. Although many existing systems can efficiently and effectively analyze specific data and tasks, no system exists that can handle all characteristics of IoT big data: volume, velocity, variety, veracity, and variability. We present some open issues and discuss the future of IoT big data analytic systems. This survey aims to help researchers and practitioners better understand current systems and develop new IoT big data analytic systems.","2327-4662","","10.1109/JIOT.2021.3131724","JSPS KAKENHI(grant numbers:JP17H06099,JP20H00584); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9631963","Distributed data processing;edge computing;smart city;spatiotemporal data;streaming processing;trajectory data","Internet of Things;Sensors;Big Data;Data analysis;Smart cities;Batch production systems;Meteorology","Big Data;data analysis;data handling;Internet of Things;parallel processing","IoT big data analytic systems;diverse IoT data;Spark-based batch processing systems;edge-aware stream processing systems;Internet of Things;business decisions;Hadoop","","17","","113","CCBY","1 Dec 2021","","","IEEE","IEEE Journals"
"When Smart Cities Get Smarter via Machine Learning: An In-Depth Literature Review","S. S. Band; S. Ardabili; M. Sookhak; A. T. Chronopoulos; S. Elnaffar; M. Moslehpour; M. Csaba; B. Torok; H. -T. Pai; A. Mosavi","Future Technology Research Center, College of Future, National Yunlin University of Science and Technology, Douliou, Taiwan; Department of Informatics, J. Selye University, Komárom, Slovakia; Department of Computer Science, Texas A&M University at Corpus Christi, Corpus Christi, TX, USA; Department of Computer Science, The University of Texas at San Antonio, San Antonio, TX, USA; Faculty of Engineering, Applied Science and Technology, Canadian University Dubai, Dubai, United Arab Emirates; Department of Business Administration, College of Management, Asia University, Taichung, Taiwan; Institute of the Information Society, University of Public Service, Budapest, Hungary; Institute of the Information Society, University of Public Service, Budapest, Hungary; International Graduate Institute of Artificial Intelligence, National Yunlin University of Science and Technology, Douliou, Taiwan; John von Neumann Faculty of Informatics, Óbuda University, Budapest, Hungary","IEEE Access","14 Jun 2022","2022","10","","60985","61015","The manuscript represents a comeprehensive and systematic literature review on the machine learning methods in the emerging applications of the smart cities. Application domains include the essential aspects of the smart cities including the energy, healthcare, transportation, security, and pollution. The research methodology presents a state-of-the-art taxonomy, evaluation and model performance where the ML algorithms are classified into one of the following four categories: decision trees, support vector machines, artificial neural networks, and advanced machine learning methods, i.e., hybrid methods, ensembles, and Deep Learning. The study found that the hybrid models and ensembles have better performance since they exhibit both a high accuracy and low overall cost. On the other hand, the deep learning (DL) techniques had a higher accuracy than the hybrid models and ensembles, but they demanded relatively higher computation power. Moreover, all these advanced ML methods had a slower processing speed than the single methods. Likewise, the support vector machine (SVM) and decision tree (DT) generally outperformed the artificial neural network (ANN) for accuracy and other metrics. However, since the difference was negligible, it can be concluded that using either of them is appropriate.","2169-3536","","10.1109/ACCESS.2022.3181718","European Union’s Horizon 2020 Research and Innovation Programme under the Programme SASPRO 2 COFUND Marie Sklodowska-Curie(grant numbers:945478); Slovak Research and Development Agency(grant numbers:APVV-20-0261); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9792241","Smart city;big data;machine learning;ensemble;artificial intelligence;deep learning;data science;smart grid","Smart cities;Databases;Urban areas;Regression tree analysis;Medical services;Machine learning;Taxonomy","decision trees;deep learning (artificial intelligence);pattern classification;support vector machines","application domains;smart cities;state-of-the-art taxonomy;support vector machines;artificial neural network;machine learning methods;hybrid methods;hybrid models;deep learning techniques;advanced ML methods;single methods;decision tree;SVM;ANN","","19","","124","CCBY","9 Jun 2022","","","IEEE","IEEE Journals"
"End-to-End Prediction of Parcel Delivery Time With Deep Learning for Smart-City Applications","A. C. de Araujo; A. Etemad","Department of Electrical and Computer Engineering, Queen’s University, Kingston, ON, Canada; Department of Electrical and Computer Engineering, Queen’s University, Kingston, ON, Canada","IEEE Internet of Things Journal","19 Nov 2021","2021","8","23","17043","17056","The acquisition of massive data on parcel delivery motivates postal operators to foster the development of predictive systems to improve customer service. Predicting delivery times successive to being shipped out of the final depot, referred to as last-mile prediction, deals with complicating factors such as traffic, drivers’ behaviors, and weather. This work studies the use of deep learning for solving a real-world case of last-mile parcel delivery time prediction. We present our solution under the Internet-of-Things (IoT) paradigm and discuss its feasibility on a cloud-based architecture as a smart city application. We focus on a large-scale parcel data set provided by Canada Post, covering the Greater Toronto Area (GTA). We utilize an origin-destination (OD) formulation, in which routes are not available, but only the start and end delivery points. We investigate three categories of convolutional-based neural networks and assess their performances on the task. We further demonstrate how our modeling outperforms several baselines, from classical machine learning models to referenced OD solutions. We perform a thorough error analysis across the data and visualize the deep features learned to better understand the model behavior, making interesting remarks on data predictability. Our work provides an end-to-end neural pipeline that leverages parcel OD data as well as weather to accurately predict delivery durations. We believe that our system has the potential not only to improve user experience by better modeling their anticipation but also to aid last-mile postal logistics as a whole.","2327-4662","","10.1109/JIOT.2021.3077007","Innovapost Inc.; Natural Sciences and Engineering Research Council of Canada (NSERC); Ontario Centres of Excellence (OCE); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9420706","Deep learning;last mile;origin–destination (OD);parcel delivery;predictive modeling","Estimation;Logistics;Internet of Things;Deep learning;Global Positioning System;Data models;Smart transportation","cloud computing;convolutional neural nets;customer services;data analysis;deep learning (artificial intelligence);Internet;logistics;postal services;smart cities","end-to-end prediction;deep learning;smart-city applications;predictive systems;customer service;drivers;last-mile parcel delivery time prediction;Internet-of-Things paradigm;cloud-based architecture;large-scale parcel data;origin-destination formulation;convolutional-based neural networks;machine learning;OD solutions;data predictability;end-to-end neural pipeline;delivery durations;last-mile postal logistics;parcel delivery","","10","","54","IEEE","3 May 2021","","","IEEE","IEEE Journals"
"Prediction of EV Charging Behavior Using Machine Learning","S. Shahriar; A. R. Al-Ali; A. H. Osman; S. Dhou; M. Nijim","Computer Science and Engineering Department, American University of Sharjah, Sharjah, United Arab Emirates; Computer Science and Engineering Department, American University of Sharjah, Sharjah, United Arab Emirates; Electrical Engineering Department, American University of Sharjah, Sharjah, United Arab Emirates; Computer Science and Engineering Department, American University of Sharjah, Sharjah, United Arab Emirates; Electrical Engineering and Computer Science Department, Texas A&M University-Kingsville, Kingsville, TX, USA","IEEE Access","13 Aug 2021","2021","9","","111576","111586","As a key pillar of smart transportation in smart city applications, electric vehicles (EVs) are becoming increasingly popular for their contribution in reducing greenhouse gas emissions. One of the key challenges, however, is the strain on power grid infrastructure that comes with large-scale EV deployment. The solution to this lies in utilization of smart scheduling algorithms to manage the growing public charging demand. Using data-driven tools and machine learning algorithms to learn the EV charging behavior can improve scheduling algorithms. Researchers have focused on using historical charging data for predictions of behavior such as departure time and energy needs. However, variables such as weather, traffic, and nearby events, which have been neglected to a large extent, can perhaps add meaningful representations, and provide better predictions. Therefore, in this paper we propose the usage of historical charging data in conjunction with weather, traffic, and events data to predict EV session duration and energy consumption using popular machine learning algorithms including random forest, SVM, XGBoost and deep neural networks. The best predictive performance is achieved by an ensemble learning model, with SMAPE scores of 9.9% and 11.6% for session duration and energy consumptions, respectively, which improves upon the existing works in the literature. In both predictions, we demonstrate a significant improvement compared to previous work on the same dataset and we highlight the importance of traffic and weather information for charging behavior predictions.","2169-3536","","10.1109/ACCESS.2021.3103119","Computer Science and Engineering Department and the Open Access Program from the American University of Sharjah, United Arab Emirates; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9508419","Electric vehicles (EVs);charging behavior;machine learning;smart city;smart transportation","Energy consumption;Support vector machines;Machine learning;Radio frequency;Prediction algorithms;Machine learning algorithms;Predictive models","air pollution control;deep learning (artificial intelligence);electric vehicle charging;electric vehicles;energy consumption;learning (artificial intelligence);neural nets;power engineering computing;power grids;random forests;road traffic;scheduling;smart cities;support vector machines","traffic;EV charging behavior;smart transportation;smart city applications;power grid infrastructure;smart scheduling algorithms;data-driven tools;machine learning;electric vehicles;greenhouse gas emission reduction;energy consumption;random forest;SVM;XGBoost;deep neural networks;SMAPE","","29","","62","CCBY","6 Aug 2021","","","IEEE","IEEE Journals"
"Smart City Based on Open Data: A Survey","K. D. C. Adje; A. B. Letaifa; M. Haddad; O. Habachi","Mediatron Research Laboratory, SUPCOM, University of Carthage, Tunis, Tunisia; Mediatron Research Laboratory, SUPCOM, University of Carthage, Tunis, Tunisia; LIA, Avignon University, Avignon, France; LIMOS, University of Clermont Auvergne, Clermont-Ferrand, France","IEEE Access","14 Jun 2023","2023","11","","56726","56748","Open data are gold mines because they can be used to create services that develop a smart city while improving users’ living conditions. Several research works go in this direction, presenting open data impact in the smart city for some, while others have focused on data processing methods. We have therefore deemed it necessary to make a state of the art on these different issues. The particularity of our study is that it shows the link between open data and smart city in all its aspects, describing what kind of open data is suitable for the smart city, how it is important for its development, and how these open data are processed to create services. Thus, in this article, we first present a review of existing surveys since 2015. Then, we present different smart city dimensions based on open data as well as some applications, and we detail how to process these data. We end with a list of open data sources as well as some challenges and solutions related to smart city services.","2169-3536","","10.1109/ACCESS.2023.3283436","French National Research Agency(grant numbers:ANR-20-CE25-000); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10144740","Data mining;machine learning (ML);open data;smart city","Open data;Smart cities;Surveys;Data mining;Data analysis;Social factors;Big Data","data mining;open data;public administration;smart cities","data mining;data processing;open data impact;open data sources;smart city dimensions;smart city services","","","","131","CCBYNCND","6 Jun 2023","","","IEEE","IEEE Journals"
"Multiagent Reinforcement-Learning-Aided Service Function Chain Deployment for Internet of Things","Y. Zhu; H. Yao; T. Mai; W. He; N. Zhang; M. Guizani","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Sixth Research Institute, China Electronic Corporation, Beijing, China; Machine Learning Department, Mohamed Bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE","IEEE Internet of Things Journal","24 Aug 2022","2022","9","17","15674","15684","Nowadays, the compelling applications of the Internet of Things (IoT) bring unexpected economic benefits to our daily lives. But at the same time, it also poses huge challenges to service providers. Diverse proprietary hardware (i.e., firewall and code conversion) have to be deployed in networks for meeting different applications’ requirements. Recently, network functions virtualization (NFV) is considered a promising technique. In the NFV-enabled architecture, network services can be implemented via a set of orderly virtual network functions (VNFs) on standardized compute nodes, which is termed service function chains (SFCs). However, with the explosion of IoT applications, embedding multiple SFCs in a shared NFV-enabled infrastructure becomes a challenging problem. Centralized schemes suffer from the scalability and private issue, while distributed schemes suffer from the nonconvergence problem. In this article, we propose a hybrid intelligent control architecture, which adopts the centralized training and distributed execution paradigm. A centralized critic is introduced to ease the training process of the distributed network nodes. Besides, considering the competitive behavior of users, we formulate the resource allocation problem as a multiuser competition game model. Based on this, we proposed a multiagent reinforcement learning-based SFCs deployment algorithm.","2327-4662","","10.1109/JIOT.2022.3151134","National Key Research and Development Program of China(grant numbers:2018YFB1800805); Artificial Intelligence and Smart City Joint Laboratory (BUPT-TGSTII)(grant numbers:B2020001); Future Intelligent Networking and Intelligent Transportation Joint Laboratory (BUPTCTTIC)(grant numbers:B2019007); Intelligent Network Joint Laboratory (BUPTIN)(grant numbers:B2021006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9712642","Internet of Things (IoT);multiagent system;network function virtualization;reinforcement learning (RL);service function chain (SFC)","Internet of Things;Servers;Games;Scalability;Privacy;Costs;Hardware","game theory;intelligent control;Internet;Internet of Things;learning (artificial intelligence);multi-agent systems;resource allocation;virtualisation","centralized training;distributed network nodes;multiagent reinforcement learning-based SFCs deployment algorithm;multiagent reinforcement-learning-aided service function chain deployment;compelling applications;unexpected economic benefits;proprietary hardware;firewall;code conversion;meeting different applications;network functions virtualization;NFV-enabled architecture;network services;orderly virtual network functions;standardized compute nodes;service function chains;IoT applications;multiple SFCs;shared NFV-enabled infrastructure;centralized schemes;distributed schemes;hybrid intelligent control architecture","","6","","33","IEEE","14 Feb 2022","","","IEEE","IEEE Journals"
"A Deep Learning Approach for IoT Traffic Multi-Classification in a Smart-City Scenario","A. Hameed; J. Violos; A. Leivadeas","Department of Software and Information Technology Engineering, École de Technologie Supérieure, Montreal, QC, Canada; Department of Software and Information Technology Engineering, École de Technologie Supérieure, Montreal, QC, Canada; Department of Software and Information Technology Engineering, École de Technologie Supérieure, Montreal, QC, Canada","IEEE Access","2 Mar 2022","2022","10","","21193","21210","As the number of Internet of Things (IoT) devices and applications increases, the capacity of the IoT access networks is considerably stressed. This can create significant performance bottlenecks in various layers of an end-to-end communication path, including the scheduling of the spectrum, the resource requirements for processing the IoT data at the Edge and/or Cloud, and the attainable delay for critical emergency scenarios. Thus, a proper classification or prediction of the time varying traffic characteristics of the IoT devices is required. However, this classification remains at large an open challenge. Most of the existing solutions are based on machine learning techniques, which nonetheless present high computational cost, whereas they are not considering the fine-grained flow characteristics of the traffic. To this end, this paper introduces the following four contributions. Firstly, we provide an extended feature set including, flow, packet and device level features to characterize the IoT devices in the context of a smart environment. Secondly, we propose a custom weighting based preprocessing algorithm to determine the importance of the data values. Thirdly, we present insights into traffic characteristics using feature selection and correlation mechanisms. Finally, we develop a two-stage learning algorithm and we demonstrate its ability to accurately categorize the IoT devices in two different datasets. The evaluation results show that the proposed learning framework achieves 99.9% accuracy for the first dataset and 99.8% accuracy for the second. Additionally, for the first dataset we achieve a precision and recall performance of 99.6% and 99.5%, while for the second dataset the precission and recall attained is of 99.6% and 99.7% respectively. These results show that our approach clearly outperforms other well-known machine learning methods. Hence, this work provides a useful model deployed in a realistic IoT scenario, where IoT traffic and devices’ profiles are predicted and classified, while facilitating the data processing in the upper layers of an end-to-end communication model.","2169-3536","","10.1109/ACCESS.2022.3153331","CHIST-ERA-18-SDCDN-003-DRUID-NET Project “eDge computing ResoUrse allocatIon for Dynamic NETworks (DRUID-NET)”; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9718208","Deep learning;edge computing;Internet of Things;machine learning;neural networks;traffic classification","Internet of Things;Performance evaluation;Machine learning;Feature extraction;Deep learning;Support vector machines;Quality of service","deep learning (artificial intelligence);Internet of Things;smart cities;telecommunication scheduling;telecommunication traffic","IoT devices;traffic characteristics;feature selection;correlation mechanisms;two-stage learning algorithm;learning framework;machine learning methods;realistic IoT scenario;end-to-end communication model;deep learning approach;iot traffic multiclassification;smart-city scenario;IoT access networks;end-to-end communication path;IoT data;machine learning techniques;fine-grained flow characteristics","","9","","33","CCBY","22 Feb 2022","","","IEEE","IEEE Journals"
"Sustainable and Trustworthy Edge Machine Learning","I. Brandic","Vienna University of Technology, Vienna, Austria","IEEE Internet Computing","8 Oct 2021","2021","25","5","5","9","Nowadays, our world is driven by complex, large scale, yet tactile information systems requiring various degrees of trustworthiness. Trustworthiness of the systems always comes with costs. The traditional and rather costly way to understand the behavior of large scale systems is to develop powerful mathematical abstractions that allow us to condense these behaviors and to reason about them at a very abstract level. In our FWF funded project Rucon, we introduce an orthogonal, data driven, and probabilistic concept to model and reason uncertainty of the systems. In Rucon, deliberated system failures are tolerated due to the benefits of the costs and sustainability. Rucon's approach targets large scale near real-time systems like live video analytics, streaming, vehicular applications, and smart city information systems.","1941-0131","","10.1109/MIC.2021.3104383","Austrian Science Fund; City of Vienna; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9565455","","Costs;Edge computing;Smart cities;Visual analytics;Machine learning;Probabilistic logic;Real-time systems;Sustainable development","information systems;large-scale systems;learning (artificial intelligence);tactile sensors","tactile information systems;trustworthiness;mathematical abstractions;abstract level;FWF funded project Rucon;probabilistic concept;deliberated system failures;real-time systems;smart city information systems;sustainable trustworthy edge machine learning;Rucon approach","","","","14","IEEE","8 Oct 2021","","","IEEE","IEEE Magazines"
"Mobile Service Traffic Classification Based on Joint Deep Learning With Attention Mechanism","C. Li; C. Dong; K. Niu; Z. Zhang","Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Key Laboratory of Universal Wireless Communications, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; Smart City College, Beijing Union University, Beijing, China","IEEE Access","24 May 2021","2021","9","","74729","74738","With the rapid development of mobile devices, smartphones have become the chief access to Internet and generated huge mobile service traffic. Mobile service traffic classification (MSTC) has been an important task that contributes to providing personalized services for end-users. With the excellent ability of automatic feature learning, deep learning has better performance than traditional machine learning methods. Giving more attention to a local focus, the attention mechanism can reduce computational complexity by filtering out useless information. Therefore, deep learning with attention mechanism can effectively realize automatic feature learning and reduce computational complexity. In this paper, a novel method for MSTC with a two-step strategy is proposed, which reduces the computational complexity of the deep learning model by attention mechanism. In the first step, a joint deep learning model is designed as a basic classifier, which learns features of mobile service traffic from multiple time scales. In the second step, the attention mechanism is adopted to aggregates the basic predictions generated in the first step. To verify this methodology, an experiment is performed to classify seven mobile services. The results show that we get the mean F1-score of 92.7% with 3.1 seconds time-delay, where the pure deep learning model gets the highest mean F1-score of 90.4% with 6.7 seconds time-delay.","2169-3536","","10.1109/ACCESS.2021.3081504","National Key Research and Development Program of China(grant numbers:2018YFE0205501); Key Program of National Natural Science Foundation of China(grant numbers:92067202); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9433596","Mobile service traffic classification (MSTC);joint deep learning;attention mechanism","Deep learning;Streaming media;Telecommunication traffic;Protocols;Computational complexity;Computational modeling;Aggregates","computational complexity;deep learning (artificial intelligence);feature extraction;Internet;mobile computing;mobile handsets;pattern classification;telecommunication computing;telecommunication traffic","mobile service traffic classification;attention mechanism;mobile devices;personalized services;automatic feature learning;machine learning methods;computational complexity;joint deep learning model;mobile services;pure deep learning model;MSTC;mean F1-score;time 3.1 s;time 6.7 s","","4","","25","CCBY","17 May 2021","","","IEEE","IEEE Journals"
"Service Versus Protection: A Bayesian Learning Approach for Trust Provisioning in Edge of Things Environment","P. Singh; A. Kaur; R. S. Batth; G. S. Aujla; M. Masud","School of Computer Science and Engineering, Lovely Professional University, Phagwara, India; School of Computer Science and Engineering, Lovely Professional University, Phagwara, India; School of Computer Science and Engineering, Lovely Professional University, Phagwara, India; Department of Computer Science, Durham University, Durham, U.K.; Department of Computer Science, College of Computers and Information Technology, Taif University, Taif, Saudi Arabia","IEEE Internet of Things Journal","4 Nov 2022","2022","9","22","22061","22070","Edge of Things (EoT) technology enables end-users participation with smart sensors and mobile devices (such as smartphones and wearable devices) to the smart devices across the smart city. Trust management is the main challenge in EoT infrastructure to consider the trusted participants. The Quality of Service (QoS) is highly affected by malicious users with fake or altered data. In this article, a robust trust management (RTM) scheme is designed based on Bayesian learning and collaboration filtering. The proposed RTM model is regularly updated after a specific interval with the significant decay value to the current calculated scores to update the behavior changes quickly. The dynamic characteristics of edge nodes are analyzed with the new probability score mechanism from recent services’ behavior. The performance of the proposed trust management scheme is evaluated in a simulated environment. The percentage of collaboration devices is tuned as 10%, 50%, and 100%. The maximum accuracy of 99.8% is achieved from the proposed RTM scheme. The experimental results demonstrate that the RTM scheme shows better performance than the existing techniques in filtering malicious behavior and accuracy.","2327-4662","","10.1109/JIOT.2021.3082272","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9437344","Edge of Things (EoT);machine learning;malicious attack;smart city;smart sensors;trust management","Trust management;Internet of Things;Smart cities;Computational modeling;Peer-to-peer computing;Data models;Analytical models","Bayes methods;collaborative filtering;Internet of Things;learning (artificial intelligence);mobile computing;quality of service;security of data;trusted computing","Bayesian learning;collaboration devices;collaboration filtering;edge nodes;Edge of Things;end-users participation;EoT infrastructure;mobile devices;probability score mechanism;QoS;quality of service;robust trust management;RTM;smart devices;smart sensors;trust provisioning;trusted participants","","5","","40","IEEE","20 May 2021","","","IEEE","IEEE Journals"
"Preemptive Scheduling for Distributed Machine Learning Jobs in Edge-Cloud Networks","N. Wang; R. Zhou; L. Jiao; R. Zhang; B. Li; Z. Li","School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science and the Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Department of Computer and Information Science, University of Oregon, Eugene, OR, USA; School of Computer Science and the Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; School of Computer Science, Wuhan University, Wuhan, China","IEEE Journal on Selected Areas in Communications","15 Jul 2022","2022","40","8","2411","2425","Recent advances in 5G and edge computing enable rapid development and deployment of edge-cloud systems, which are ideal for delay-sensitive machine learning (ML) applications such as autonomous driving and smart city. Distributed ML jobs often need to train a large model with enormous datasets, which can only be handled by deploying a distributed set of workers in an edge-cloud system. One common approach is to employ a parameter server (PS) architecture, in which training is carried out at multiple workers, while PSs are used for aggregation and model updates. In this architecture, one of the fundamental challenges is how to dispatch ML jobs to workers and PSs such that the average job completion time (JCT) can be minimized. In this work, we propose a novel online preemptive scheduling framework to decide the location and the execution time window of concurrent workers and PSs upon each job arrival. Specifically, our proposed scheduling framework consists of: i) a job dispatching and scheduling algorithm that assigns each ML job to workers and decides the schedule to train each data chunk; ii) a PS assignment algorithm that determines the placement of PS. We prove theoretically that our proposed algorithm is  $D_{max}(1+1/\epsilon)$ -competitive with  $(1 + \epsilon)$ -speed augmentation, where  $D_{max}$  is the maximal number of data chunks in any job. Extensive testbed experiments and trace-driven simulations show that our algorithm can reduce the average JCT by up to 30% compared with state-of-the-art baselines.","1558-0008","","10.1109/JSAC.2022.3180772","NSFC(grant numbers:62072344,U20A20177); Hubei Science Foundation(grant numbers:2020CFB195); Compact Exponential Algorithm Project of Huawei(grant numbers:YBN2020035131); Huawei(grant numbers:FA2019071041); U.S. National Science Foundation(grant numbers:CNS-2047719); Research Grant Committee (RGC) Research Impact Fund (RIF)(grant numbers:R6021-20); RGC General Research Fund (GRF) Grants(grant numbers:16209120,16200221); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9791406","Distributed machine learning;parameter server architecture;preemptive scheduling;edge-cloud networks","Servers;Training;Data models;Resource management;Parallel processing;Machine learning;Computer science","cloud computing;computational complexity;learning (artificial intelligence);minimisation;scheduling","distributed machine learning jobs;edge-cloud networks;edge computing;edge-cloud system;delay-sensitive machine;autonomous driving;smart city;distributed ML jobs;enormous datasets;distributed set;parameter server architecture;multiple workers;PSs;ML job;average job completion time;online preemptive scheduling framework;execution time window;concurrent workers;job arrival;job dispatching;scheduling algorithm;PS assignment algorithm;max-competitive with-speed augmentation;1 + ε","","","","46","IEEE","8 Jun 2022","","","IEEE","IEEE Journals"
"Opportunities, Applications, and Challenges of Edge-AI Enabled Video Analytics in Smart Cities: A Systematic Review","E. Badidi; K. Moumane; F. E. Ghazi","Department of Computer Science and Software Engineering, CIT, UAE University, Al Ain, United Arab Emirates; Software Project Management Research Team, ENSIAS, Mohammed V University, Rabat, Morocco; Physics and Applications Laboratory, Ibn Tofail University, Kenitra, Morocco","IEEE Access","7 Aug 2023","2023","11","","80543","80572","Video analytics with deep learning techniques has generated immense interest in academia and industry, captivating minds with its transformative potential. Deep learning techniques and the deluge of video data enable the mechanization of tasks that were once the exclusive domain of human effort. Furthermore, edge intelligence is emerging as an interdisciplinary technology that drives the fusion of edge computing and artificial intelligence (AI). Edge computing allows the Internet of Things (IoT) devices with limited resources to offload their compute-intensive AI applications to the network edge servers for execution. Specifically, AI workloads for video analytics can be moved to the network edge from the cloud, providing improved latency and bandwidth savings, among other benefits. This article reviews current technologies used in Edge AI-assisted video analytics in smart cities. It examines the various artificial intelligence models and privacy-preserving techniques used in edge video analytics. It identifies the various applications of video analytics in smart cities, including security and surveillance, transportation and traffic management, healthcare, education, sports and entertainment, and many more. Besides, it highlights the challenges of edge video analysis and open research issues. It is expected that this review will be valuable for researchers, engineers, and decision-makers who want to understand the landscape and scale of edge video analytics in smart cities.","2169-3536","","10.1109/ACCESS.2023.3300658","United Arab Emirates (UAE) University—UAEU Program for Advanced Research (UPAR)(grant numbers:G00003443); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10198424","Artificial intelligence;deep learning;edge computing;edge intelligence;edge video analytics;machine learning;smart city","Visual analytics;Security;Smart cities;Edge computing;Artificial intelligence;Streaming media;Internet of Things;Machine learning;Deep learning","","","","","","178","CCBY","1 Aug 2023","","","IEEE","IEEE Journals"
"An Attention-Based Deep Learning Framework for Trip Destination Prediction of Sharing Bike","W. Wang; X. Zhao; Z. Gong; Z. Chen; N. Zhang; W. Wei","State Key Laboratory of Internet of Things for Smart City, University of Macau, Macau, China; School of Management Engineering and Business, Hebei University of Engineering, Handan, China; State Key Laboratory of Internet of Things for Smart City, University of Macau, Macau, China; School of Software, Dalian University of Technology, Dalian, China; Department of Computing Sciences, Texas A&M University at Corpus Christi, Corpus Christi, TX, USA; School of Computer Science and Engineering, Xi’an University of Technology, Xi’an, China","IEEE Transactions on Intelligent Transportation Systems","12 Jul 2021","2021","22","7","4601","4610","With the advancement of communication technology and location acquisition technology in the context of modern smart cities, the sharing bike systems offer users the great autonomy and convenience for the last/first-kilometer trip. Meanwhile, we can now able to collect, store, and analyze a large amount of sharing bike data. How to effectively use these massive data to provide better services is an emerging task. However, due to the skewed and imbalanced bike usages for stations located at different places, it is of great significance yet very challenging to predict the potential destinations of each individual trip beforehand so that the service providers can better schedule manual bike re-dispatch in advance. To address this issue, this paper proposes an attention-based deep learning framework for trip destination prediction (AFTER). AFTER first learns the low-dimension representations of users and sharing bike stations via negative sampling strategies. Then, a convolution neural network with an attention mechanism is utilized to predict the future trip destination. Experimental results on a real-world dataset indicate that the proposed framework outperforms several state-of-the-art approaches in terms of precision, recall, and F1.","1558-0016","","10.1109/TITS.2020.3008935","Science and Technology Development Fund, Macau SAR(grant numbers:SKL-IOTSC-2018-2020,FDCT/0045/2019/A1,FDCT/007/2016/AFJ); Guangzhou Science and Technology Innovation and Development Commission(grant numbers:EF005/FST-GZG/2019/GSTIC); Research Committee of University of Macau(grant numbers:MYRG2017-00212-FST,MYRG2018-00129-FST); China Postdoctoral Science Foundation(grant numbers:2019M651115); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9152145","Sharing bike system;trip destination prediction;convolution neural networks;attention model","Machine learning;Task analysis;Neural networks;Smart cities;Convolution;Predictive models;Internet of Things","bicycles;convolutional neural nets;data analysis;deep learning (artificial intelligence);location based services;public transport;sampling methods;smart cities;town and country planning;traffic engineering computing","trip destination prediction;communication technology;location acquisition technology;smart cities;sharing bike systems;attention based deep learning;sharing bike stations;bike data analysis;negative sampling;convolution neural network","","31","","33","IEEE","29 Jul 2020","","","IEEE","IEEE Journals"
"Transfer Reinforcement Learning Aided Distributed Network Slicing Optimization in Industrial IoT","T. Mai; H. Yao; N. Zhang; W. He; D. Guo; M. Guizani","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Sixth Research Institute of China Electronic Corporation, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Machine Learning Department, MBZUAI, Abu Dhabi, UAE","IEEE Transactions on Industrial Informatics","21 Feb 2022","2022","18","6","4308","4316","With the growth of the number of Internet of Things (IoT) devices and the emergence of new applications, satisfying distinct QoS in the same physical network becomes more challenging. Recently, with the advance of network functions virtualization and software-defined networking (SDN) technologies, the network slicing technique has emerged as a promising solution. It can divide a physical network into multiple virtual networks, therefore providing different network services. In this article, to meet distinct QoS in industrial IoT, we design a network slicing architecture over the SDN-based long-range wide area network. The SDN controller can dynamically split the network into multiple virtual networks according to different business requirements. On this basis, we proposed a deep deterministic policy gradient (DDPG) based slice optimization algorithm. It enables LoRa gateways to intelligently configure slice parameters (e.g., transmission power and spreading factor) to improve the slice performance in terms of QoS, energy efficiency, and reliability. In addition, to accelerate the training process across multiple LoRa gateways, we leverage the transfer learning framework and design a transfer learning-based multiagent DDPG algorithm.","1941-0050","","10.1109/TII.2021.3132136","National Key R&D Program of China(grant numbers:2018YFB1800805); Artificial Intelligence and Smart City Joint Laboratory(grant numbers:B2020001); Future Intelligent Networking and Intelligent Transportation Joint Laboratory(grant numbers:B2019007); Intelligent Network Joint Laboratory(grant numbers:B2021006); BUPT Excellent Ph.D Student Foundation(grant numbers:CX2020108); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9633170","Industrial Internet of Things (IoT);multiagent reinforcement learning;network slicing;transfer learning","Logic gates;Network slicing;Quality of service;Optimization;Measurement;Network servers;Throughput","gradient methods;Internet of Things;internetworking;multi-agent systems;optimisation;quality of service;reinforcement learning;software defined networking;virtualisation;wide area networks","network slicing architecture;multiple virtual networks;deep deterministic policy gradient based slice optimization algorithm;slice parameters;slice performance;transfer learning framework;transfer learning-based multiagent DDPG algorithm;transfer reinforcement learning aided distributed network slicing optimization;industrial IoT;Internet of Things devices;distinct QoS;physical network;network functions virtualization;network slicing technique;network services;SDN-based long-range wide area network","","19","","24","IEEE","2 Dec 2021","","","IEEE","IEEE Journals"
"Non-Orthogonal Multiple Access Assisted Federated Learning via Wireless Power Transfer: A Cost-Efficient Approach","Y. Wu; Y. Song; T. Wang; L. Qian; T. Q. S. Quek","Department of Computer Information Science, State Key Laboratory of Internet of Things for Smart City, University of Macau, Macau, China; Department of Computer Information Science, State Key Laboratory of Internet of Things for Smart City, University of Macau, Macau, China; Department of Computer Information Science, State Key Laboratory of Internet of Things for Smart City, University of Macau, Macau, China; College of Information Engineering, Zhejiang University of Technology, Hangzhou, China; Information Systems Technology and Design Pillar, Singapore University of Technology and Design, Singapore","IEEE Transactions on Communications","15 Apr 2022","2022","70","4","2853","2869","Federated learning (FL) has been considered as a promising paradigm for enabling distributed training/learning in many machine-learning services without revealing users’ local data. Driven by the growing interests in exploiting FL in wireless networks, this paper studies the Non-orthogonal Multiple Access (NOMA) assisted FL in which a group of end-devices (EDs) form a NOMA cluster to send their locally trained models to the cellular base station (BS) for model aggregation. In particular, we consider that the BS adopts wireless power transfer (WPT) to power the EDs (for their data transmission and local training) in each round of FL iteration, and formulate a joint optimization of the BS’s WPT for different EDs, the EDs’ NOMA-transmission for sending the local models to the BS, the BS’s broadcasting of the aggregated model to all EDs, the processing-rates of the BS and EDs, as well as the training-accuracy of the FL, with the objective of minimizing the system-wise cost accounting for the total energy consumption as well as the FL convergence latency. In spite of the strict non-convexity of the joint optimization problem, we analytically characterize the BS’s and all EDs’ optimal processing-rates, based on which we propose a layered algorithm for finding the optimal solutions for the joint optimization problem via exploiting monotonic optimization. Numerical results validate that our algorithm can achieve the optimal solution as LINGO’s global-solver (i.e., a commercial optimization package) while significantly reducing the computation-time. Moreover, the results also demonstrate that our NOMA assisted FL can reduce the system cost compared to the benchmark FL scheme with the fixed local training-accuracy by more than 70% and the conventional frequency division multiple access (FDMA) based FL by 78%.","1558-0857","","10.1109/TCOMM.2022.3153068","Science and Technology Development Fund of Macau SAR(grant numbers:0060/2019/A1,0162/2019/A3); FDCT-MOST Joint Project(grant numbers:0066/2019/AMJ); National Natural Science Foundation of China(grant numbers:62122069,62071431); Intergovernmental International Cooperation in Science and Technology Innovation Program(grant numbers:2019YFE0111600); Research Grant of University of Macau(grant numbers:MYRG2020-00107-IOTSC); National Research Foundation, Singapore and Infocomm Media Development Authority under its Future Communications Research & Development Programme, and SUTD Growth Plan Grant for AI; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9718086","Federated learning (FL);non-orthogonal multiple access (NOMA);wireless power transfer (WPT);resource allocations","NOMA;Resource management;Training;Optimization;Convergence;Mathematical models;Energy consumption","cellular radio;concave programming;data aggregation;learning (artificial intelligence);nonorthogonal multiple access;radiofrequency power transmission;telecommunication computing","wireless power transfer;machine-learning services;wireless networks;NOMA cluster;locally trained models;cellular base station;model aggregation;data transmission;FL iteration;BS broadcasting;aggregated model;system-wise cost accounting;joint optimization problem;monotonic optimization;optimization package;fixed local training-accuracy;frequency division multiple access;NOMA assisted FL scheme;ED optimal processing-rates;benchmark FL convergence scheme;ED NOMA;BS WPT;nonorthogonal multiple access assisted federated learning;LINGO global-solver;FDMA","","40","","42","IEEE","21 Feb 2022","","","IEEE","IEEE Journals"
"A Review on Digital Twin Technology in Smart Grid, Transportation System and Smart City: Challenges and Future","M. Jafari; A. Kavousi-Fard; T. Chen; M. Karimi","Department of Electrical and Electronics Engineering, Shiraz University of Technology, Shiraz, Iran; Department of Electrical and Electronics Engineering, Shiraz University of Technology, Shiraz, Iran; School of Electrical Engineering, Southeast University, Nanjing, China; School of Technology and Innovations, University of Vaasa, Vaasa, Finland","IEEE Access","24 Feb 2023","2023","11","","17471","17484","With recent advances in information and communication technology (ICT), the bleeding edge concept of digital twin (DT) has enticed the attention of many researchers to revolutionize the entire modern industries. DT concept refers to a digital representation of a physical entity that is able to reflect its physical behavior by applying platforms and bidirectional interaction of data in real-time. The remarkable deployment of the internet of things in the power grid has led to reliable access to information that improves its performance and equips it with a powerful tool for real-time data management and analysis. This paper aims to trace the continuous investigation and propose practical ideas in originating and developing DT technology, according to various application domains of power systems, and also describes the proposed solutions to deal with the challenges associated with DT. Indeed, with the development of modern cities, different energy layers such as transportation systems, smart grids, and microgrids have emerged facing various issues that challenge the multi-dimensional energy management system. For example, in transportation systems, traffic is a major problem that requires real-time management, planning, and analysis. In power grids, remote data transfer within the grid and also various analyzes needing real data are just some of the current challenges in the field. These problems can be cracked by providing and analyzing a real twin framework in each section. All in all, this paper aims to survey different applications of DT in the development of the various aspects of energy management within a city including transportation systems, power grids, and microgrids. Besides, the security of DT technology based on ML is discussed. It also provides a complete view for the readers to be able to develop and deploy a DT technology for various power system applications.","2169-3536","","10.1109/ACCESS.2023.3241588","University of Vaasa, Project by Business Finland(grant numbers:27081089141); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10034656","Digital twin;machine learning;microgrid;physical twin;power system;security;transportation system","Transportation;Power systems;Real-time systems;Digital twins;Security;Microgrids;Behavioral sciences","digital twins;distributed power generation;energy management systems;Internet of Things;power engineering computing;power grids;production engineering computing;smart cities;smart power grids","bleeding edge concept;city including transportation systems;communication technology;different energy layers;digital representation;digital twin technology;DT concept;DT technology;modern cities;multidimensional energy management system;physical behavior;physical entity;power grid;power system applications;power systems;real-time data management;real-time management;remote data transfer;smart city;smart grid;transportation system;twin framework","","16","","92","CCBYNCND","1 Feb 2023","","","IEEE","IEEE Journals"
"Demystifying Thermal Comfort in Smart Buildings: An Interpretable Machine Learning Approach","W. Zhang; Y. Wen; K. J. Tseng; G. Jin","Infocomm Technology Cluster, Singapore Institute of Technology, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Engineering Cluster, Singapore Institute of Technology, Singapore; Building and Construction Authority, Built Environment Research and Innovation Institute, Singapore","IEEE Internet of Things Journal","7 May 2021","2021","8","10","8021","8031","Thermal comfort is a key consideration in smart buildings and a number of comfort models are available nowadays to evaluate the comfort level of occupants. However, the models are often complex and hardly interpretable for the developers and operators. Indeed, the model interpretations are beneficial in multifold such as for system inspection and optimization. In this article, we propose an interpretable thermal comfort system to introduce interpretability to any black-box comfort models. First, we focus on the relationship between a model’s input features and output comfort level. The feature impact on comfort is investigated and the impact patterns are shown to be diverse for different features. Second, we unveil the model mechanisms about the data processing inside the model by building the model surrogates based on the interpretable machine learning algorithms. The surrogates offer outstanding fidelity for simulating the actual model mechanisms and the interpretations based on the surrogates are intuitive and informative. Our interpretable comfort system can be integrated with the existing building management systems. Accordingly, we can ease building owner’s concerns about adopting new black-box technologies and enable various smart building applications like smart energy management.","2327-4662","","10.1109/JIOT.2020.3042783","National Research Foundation (NRF) through the Green Buildings Innovation Cluster, administered by Building and Construction Authority Singapore(grant numbers:NRF2015ENC_GBICRD001-012); National Research Foundation through the Behavioral Studies in Energy, Water, Waste, and Transportation Sectors, administered by National University of Singapore(grant numbers:BSEWWT2017_2_06); Nanyang Technological University (NTU) through the Data Science and Artificial Intelligence Research Centre @ NTU(grant numbers:DSAIR@NTU); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284464","Deep learning;interpretable machine learning (ML);smart building;smart city;thermal comfort","Data models;Atmospheric modeling;Smart buildings;Internet of Things;Computational modeling;Systems architecture;Indexes","building management systems;buildings (structures);learning (artificial intelligence);structural engineering computing;thermal comfort","smart buildings;black-box comfort models;machine learning;building management systems;smart energy management;thermal comfort","","12","","23","IEEE","7 Dec 2020","","","IEEE","IEEE Journals"
"Situation Awareness of Energy Internet of Things in Smart City Based on Digital Twin: From Digitization to Informatization","X. He; Q. Ai; J. Wang; F. Tao; B. Pan; R. Qiu; B. Yang","Faculty of Electric Power Engineering, Shanghai Jiao Tong University, Shanghai, China; Faculty of Electric Power Engineering, Shanghai Jiao Tong University, Shanghai, China; Faculty of Electric Power Engineering, Kunming University of Science and Technology, Kunming, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; State Grid Shanghai Municipal Electric Power Company, Shanghai, China; School of Electronic Information and Communication, Huazhong University of Science and Technology, Wuhan, China; Faculty of Electric Power Engineering, Kunming University of Science and Technology, Kunming, China","IEEE Internet of Things Journal","21 Apr 2023","2023","10","9","7439","7458","Rapid growth of diversity, uncertainty, and coupling effect of units in modern energy systems jointly challenges the traditional model-based situation awareness (SA) in Energy Internet of Things (EIoT). This work explores the digital twin of EIoT (EIoT-DT) and then provides a novel data-driven SA paradigm, named DT-SA, as a promising alternative. Based on the combination of the latest data technologies and machine learning algorithms, DT-SA transfers those stubborn SA challenges to digital space, and then addresses them by building a domain-specific and data-friendly digital twin (DT) model upon massive data. The established model can be quantitatively tested via iterative virtual–real interaction and, thus, be evaluated and updated through closed-loop feedback to improve its performance in the physical world. To this end, some engineering and scientific problems are raised: 1) virtual–real interaction mechanism relevant to resource flow and data flow; 2) unified modeling and analysis of heterogeneous spatial–temporal data; 3) DT configuration and evolution; and 4) domain-specific DT-SA characterization. To solve these problems, cloud-edge-terminal configuration, big data analytics (BDA), DT, and SA indicator systems are studied, respectively. Then, the random matrix theory (RMT) and overarching DT-SA framework are designed as a roadmap. Besides, some potential applications and undergoing projects on the terminal, edge, or cloud are discussed, e.g., condition assessment of equipment, digital monitoring and diagnosis of the power grid network, and EIoT construction in the smart city. Finally, some perspectives and recommendations are proposed in conclusion for future research. This research can be regarded as an efficient handbook for both energy engineering and data science, which may benefit enterprise digitization, smart city, etc.","2327-4662","","10.1109/JIOT.2022.3203823","National Natural Science Foundation of China(grant numbers:61963020,51907121); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9875220","Data driven;digital twin (DT);framework;high-dimensional indicator;jointly spatial-temporal analysis;situation awareness (SA);uncertainty","Data models;Internet of Things;Uncertainty;Smart cities;Systematics;Digital twins;Complexity theory","Big Data;cloud computing;cyber-physical systems;data analysis;digital twins;energy Internet;Internet of Things;learning (artificial intelligence);power engineering computing;power grids;smart cities","closed-loop feedback;cloud-edge-terminal configuration;data flow;data science;data-friendly;digital monitoring;digital space;digital twin;domain-specific;DT-SA framework;EIoT construction;EIoT-DT;energy engineering;Energy Internet;enterprise digitization;latest data technologies;machine learning algorithms;massive data;modern energy systems;named DT-SA;novel data-driven SA paradigm;scientific problems;smart city;spatial-temporal data;stubborn SA challenges;traditional model-based situation awareness;virtual-real interaction mechanism","","3","","68","IEEE","2 Sep 2022","","","IEEE","IEEE Journals"
"A Stable AI-Based Binary and Multiple Class Heart Disease Prediction Model for IoMT","X. Yuan; J. Chen; K. Zhang; Y. Wu; T. Yang","Qinhuangdao Branch Campus, Northeastern University, Qinhuangdao, China; Qinhuangdao Branch Campus, Northeastern University, Qinhuangdao, China; Department of Electrical and Computer Engineering, University of Nebraska–Lincoln, Omaha, NE, USA; State Key Laboratory of Internet of Things for Smart City and the Department of Computer and Information Science, University of Macau, Taipa, Macau SAR, China; Navigation College, Dalian Maritime University, Dalian, China","IEEE Transactions on Industrial Informatics","7 Dec 2021","2022","18","3","2032","2040","Heart disease seriously threatens human life due to high morbidity and mortality. Accurate prediction and diagnosis become more critical for early prevention, detection, and treatment. The Internet of Medical Things and artificial intelligence support healthcare services in heart disease monitoring, prediction, and diagnosis. However, most prediction models only predict whether people are sick, and rarely further determine the severity of the disease. In this article, we propose a machine learning based prediction model to achieve binary and multiple classification heart disease prediction simultaneously. We first design a Fuzzy-GBDT algorithm combining fuzzy logic and gradient boosting decision tree (GBDT) to reduce data complexity and increase the generalization of binary classification prediction. Then, we integrate Fuzzy-GBDT with bagging to avoid overfitting. The Bagging-Fuzzy-GBDT for multiclassification prediction further classify the severity of heart disease. Evaluation results demonstrate the Bagging-Fuzzy-GBDT has excellent accuracy and stability in both binary and multiple classification predictions.","1941-0050","","10.1109/TII.2021.3098306","National Natural Science Foundation of China(grant numbers:61901099,61972076,61973069); Science and Technology Development Fund of Macau SAR(grant numbers:0060/2019/A1); Natural Science Foundation of Hebei Province(grant numbers:F2020501037); National Key R&D Program of China(grant numbers:2020YFB1806800); National Defense Foundation Strengthening Plan(grant numbers:2020-JCJQ-ZD-020-05); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9517094","Fuzzy logic;gradient boosting decision tree (GBDT);heart disease predication and diagnosis;Internet of Medical Things (IoMT);machine learning","Diseases;Heart;Prediction algorithms;Predictive models;Data models;Decision trees;Machine learning algorithms","artificial intelligence;cardiology;data mining;decision trees;diseases;fuzzy logic;fuzzy set theory;health care;Internet of Things;learning (artificial intelligence);medical computing;medical diagnostic computing;patient diagnosis;pattern classification","stable AI-based binary;multiple class heart disease prediction model;human life;high morbidity;mortality;artificial intelligence support healthcare services;heart disease monitoring;binary classification heart disease prediction;multiple classification heart disease prediction;Fuzzy-GBDT algorithm;fuzzy logic;gradient boosting decision tree;binary classification prediction;multiclassification prediction;binary classification predictions;multiple classification predictions;bagging-fuzzy-GBDT;Internet of medical things","","25","","27","IEEE","18 Aug 2021","","","IEEE","IEEE Journals"
"Involvement of Surveillance Drones in Smart Cities: A Systematic Review","A. Gohari; A. B. Ahmad; R. B. A. Rahim; A. S. M. Supa’at; S. Abd Razak; M. S. M. Gismalla","Department of Geoinformation, Faculty of Built Environment and Surveying, Universiti Teknologi Malaysia, Skudai, Johor, Malaysia; Department of Geoinformation, Faculty of Built Environment and Surveying, Universiti Teknologi Malaysia, Skudai, Johor, Malaysia; Faculty of Engineering, School of Electrical Engineering, Universiti Teknologi Malaysia, Skudai, Johor, Malaysia; Faculty of Engineering, School of Electrical Engineering, Universiti Teknologi Malaysia, Skudai, Johor, Malaysia; Faculty of Engineering, School of Computing, Universiti Teknologi Malaysia, Skudai, Johor, Malaysia; Faculty of Engineering, School of Electrical Engineering, Universiti Teknologi Malaysia, Skudai, Johor, Malaysia","IEEE Access","2 Jun 2022","2022","10","","56611","56628","Drones, or unmanned aerial vehicles (UAVs), are among the most beneficial and emerging technologies, with a wide range of applications that can support the sustainability concerns of smart cities and ultimately improve citizens’ quality of life. The goals of this systematic review were to explore the involvement of surveillance drones in smart cities in terms of application status, application areas, proposed models, and characteristics of drones. We conducted this systematic review based on the preferred reporting items for systematic reviews and meta-analyzes (PRISMA) guidelines. We systematically searched the Web of Science and Scopus for journal articles and conference papers written in English and published up to August 2021. Of the 323 records identified, 43 met the inclusion criteria. Findings showed that surveillance drones were used in seven distinct research fields (transportation, environment, infrastructure, object or people detection, disaster management, data collection, and other applications). Air pollution and traffic monitoring were the dominant application areas. The majority of reviewed models were based on the application of rotary-wing single-drones with the camera as the aerial sensor. Reviewed models showed that the adoption of a single or multiple UAVs, either as a stand-alone technology or integrated with other technologies (e.g., internet of things, wireless sensor networks, convolutional neural networks, artificial intelligence, machine learning, computer vision, cloud computing, web applications), can offer efficient and sustainable solutions compared to conventional surveillance methods. This review can benefit academic researchers and practitioners.","2169-3536","","10.1109/ACCESS.2022.3177904","Universiti Teknologi Malaysia (UTM) Research Alliance (RA) ICONIC Grant, UTM(grant numbers:Q.J130000.4352.09G75); Research Management Centre (RMC) through the Professional Development Research University, UTM, Malaysia(grant numbers:05E69); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9781426","Applications;drone;smart city;surveillance;sensor;review","Smart cities;Drones;Surveillance;Systematics;Monitoring;Autonomous aerial vehicles;Statistics","autonomous aerial vehicles;cloud computing;robot vision;smart cities;surveillance","systematic review;unmanned aerial vehicles;beneficial emerging technologies;smart cities;surveillance drones;application status;dominant application areas;rotary-wing single-drones;Web applications;UAV;people detection;disaster management;data collection;air pollution;traffic monitoring;aerial sensor;PRISMA guidelines","","16","","60","CCBY","25 May 2022","","","IEEE","IEEE Journals"
"Generalizing AI: Challenges and Opportunities for Plug and Play AI Solutions","I. A. Ridhawi; S. Otoum; M. Aloqaily; A. Boukerche",Kuwait College of Science and Technology; Zayed University; Al Ain University; University of Ottawa,"IEEE Network","16 Feb 2021","2021","35","1","372","379","Artificial Intelligence (AI) has revolutionized today's Internet of Things (IoT) applications and services by introducing significant technological enhancements across a multitude of domains. With the deployment of the fifth generation (5G) mobile communication network, smart city visions of fast, on-demand, intelligent user-specific services are now becoming a reality. The concept of connected IoT is evolving into connected intelligent things. The advancements of both AI techniques, coupled with the sophistication of edge devices, is now leading to a new era of connected intelligence. Moving the intelligence toward end devices must account for latency demands and simplicity of selecting the type of AI technique to be used. Moreover, since most AI techniques require learning from big data sets and reasoning using a multitude of classification patterns, new simplified and collaborative solutions are now necessary more than ever. As such, the concept of introducing decentralized and distributed `Plug and Play' (PnP) AI tools is now becoming more attractive given the vast numbers in edge devices, data volume and AI techniques. To this end, this article envisions a novel general AI solution that can be adapted to autonomously select the type of machine learning (ML) algorithm, the data set to be used, and provide reasoning in regards to data selection for optimal features extraction. Moreover, the solution performs the necessary training and all the necessary parameter fine-tunings to achieve the highest level of generality and simplicity for AI at the edge. We explore several aspects related to PnP-AI and its impact in the smart city ecosystem.","1558-156X","","10.1109/MNET.011.2000371","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9210129","","Data models;Adaptation models;Machine learning;Training data;Machine learning algorithms;Support vector machines","feature extraction;generalisation (artificial intelligence);Internet;Internet of Things;learning (artificial intelligence);mobile communication","generalizing AI;Artificial Intelligence;fifth generation mobile communication network;intelligent user-specific services;connected IoT;connected intelligent things;AI technique;edge devices;PnP-AI;Plug and Play AI solutions;general AI solution;5G","","38","","15","IEEE","30 Sep 2020","","","IEEE","IEEE Magazines"
"Deep Q-Network-Based Feature Selection for Multisourced Data Cleaning","Q. Wang; Y. Guo; L. Yu; X. Chen; P. Li","Department of Electrical, Computer, and Systems Engineering, Case Western Reserve University, Cleveland, OH, USA; Department of Electrical, Computer, and Systems Engineering, Case Western Reserve University, Cleveland, OH, USA; Department of Electrical, Computer, and Systems Engineering, Case Western Reserve University, Cleveland, OH, USA; College of Aeronautics and Engineering, Kent State University, Kent, OH, USA; Department of Electrical, Computer, and Systems Engineering, Case Western Reserve University, Cleveland, OH, USA","IEEE Internet of Things Journal","25 Oct 2021","2021","8","21","16153","16164","The Internet of Things (IoT) integrates information collected from multisources and is able to support various intelligent smart city applications, such as industrial manufacturing, power systems, and mobile healthcare. In the big data era, multisourced data are collected on a daily basis, whereas a large part of the data may be irrelevant, redundant, noisy, or even malicious from a machine learning perspective. Feature selection has been a powerful data cleaning technique to reduce data redundancy and improve system performance in machine learning. Inspired by reinforcement learning that learns from its experience, in this article, we propose a novel efficient deep  $Q$ -network (DQN)-based feature selection method for multisourced data cleaning. In particular, we model the feature selection problem as a competition between an agent and the environment in dynamic states, which is solved by a DQN. Traditional DQN suffers from high computational complexity and requires a significant amount of time in order to converge in the training process. To tackle these challenges, we develop a space searching algorithm called SS to speed up the training process of the DQN agent. To validate the efficacy and efficiency of the proposed method, we conduct extensive experiments on various types of IoT data. Simulation results show that the proposed DQN-based feature selection algorithms achieve much better performance compared with state-of-the-art methods, and are robust under data poisoning attacks.","2327-4662","","10.1109/JIOT.2020.3016297","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9166522","Deep Q-network (DQN);feature selection;multisourced data;reinforcement learning","Feature extraction;Machine learning;Internet of Things;Smart cities;Training;Mutual information;Correlation","Big Data;computational complexity;computer network security;data handling;Internet of Things;learning (artificial intelligence);search problems","multisourced data cleaning;multisources;intelligent smart city applications;power systems;big data era;machine learning perspective;powerful data cleaning technique;data redundancy;reinforcement learning;novel efficient deep Q-network-based feature selection method;feature selection problem;IoT data;DQN-based feature selection algorithms;data poisoning attacks","","5","","51","IEEE","13 Aug 2020","","","IEEE","IEEE Journals"
"Adversarial Caching Training: Unsupervised Inductive Network Representation Learning on Large-Scale Graphs","J. Chen; Z. Gong; W. Wang; C. Wang; Z. Xu; J. Lv; X. Li; K. Wu; W. Liu","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Department of Computer Information Science, State Key Laboratory of Internet of Things for Smart City, University of Macau, Macau, China; School of Intelligent Systems Engineering, Sun Yat-sen University, Guangzhou, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; State Key Laboratory of Reliability and Intelligence of Electrical Equipment, Hebei University of Technology, Tianjin, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Transactions on Neural Networks and Learning Systems","30 Nov 2022","2022","33","12","7079","7090","Network representation learning (NRL) has far-reaching effects on data mining research, showing its importance in many real-world applications. NRL, also known as network embedding, aims at preserving graph structures in a low-dimensional space. These learned representations can be used for subsequent machine learning tasks, such as vertex classification, link prediction, and data visualization. Recently, graph convolutional network (GCN)-based models, e.g., GraphSAGE, have drawn a lot of attention for their success in inductive NRL. When conducting unsupervised learning on large-scale graphs, some of these models employ negative sampling (NS) for optimization, which encourages a target vertex to be close to its neighbors while being far from its negative samples. However, NS draws negative vertices through a random pattern or based on the degrees of vertices. Thus, the generated samples could be either highly relevant or completely unrelated to the target vertex. Moreover, as the training goes, the gradient of NS objective calculated with the inner product of the unrelated negative samples and the target vertex may become zero, which will lead to learning inferior representations. To address these problems, we propose an adversarial training method tailored for unsupervised inductive NRL on large networks. For efficiently keeping track of high-quality negative samples, we design a caching scheme with sampling and updating strategies that has a wide exploration of vertex proximity while considering training costs. Besides, the proposed method is adaptive to various existing GCN-based models without significantly complicating their optimization process. Extensive experiments show that our proposed method can achieve better performance compared with the state-of-the-art models.","2162-2388","","10.1109/TNNLS.2021.3084195","National Key Research and Development Program of China(grant numbers:2019YFB1600704); Science and Technology Development Fund, Macau, SAR(grant numbers:FDCT/0068/2020/AGJ,FDCT/0045/2019/A1); Key-Area Research and Development Program of Guangdong Province(grant numbers:2019B111103001); National Natural Science Foundation of China(grant numbers:U2001207,61872248,61902249,61876065); Guangdong “Pearl River Talent Recruitment Program”(grant numbers:2019ZT08X603); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9451538","Adversarial learning;graph neural network;inductive learning;negative sampling (NS);network embedding","Generators;Adaptation models;Adversarial machine learning;Computer science;Computational modeling;Probabilistic logic","data mining;data visualisation;graph theory;learning (artificial intelligence);pattern classification;pattern clustering;unsupervised learning","adversarial caching training;adversarial training method;conducting unsupervised learning;data mining research;data visualization;existing GCN-based models;graph convolutional network-based models;graph structures;high-quality negative samples;inferior representations;large-scale graphs;learned representations;link prediction;low-dimensional space;negative sampling;negative vertices;network embedding;real-world applications;subsequent machine learning tasks;target vertex;training costs;unrelated negative samples;unsupervised inductive network representation learning;unsupervised inductive NRL;updating strategies;vertex classification;vertex proximity","","4","","32","IEEE","10 Jun 2021","","","IEEE","IEEE Journals"
"Secrecy Driven Federated Learning via Cooperative Jamming: An Approach of Latency Minimization","T. Wang; Y. Li; Y. Wu; T. Q. S. Quek","Department of Computer and Information Science, State Key Laboratory of Internet of Things for Smart City, University of Macau, Macau, China; Department of Computer and Information Science, State Key Laboratory of Internet of Things for Smart City, University of Macau, Macau, China; Department of Computer and Information Science, State Key Laboratory of Internet of Things for Smart City, University of Macau, Macau, China; Information Systems Technology and Design Pillar, Singapore University of Technology and Design, Singapore","IEEE Transactions on Emerging Topics in Computing","5 Dec 2022","2022","10","4","1687","1703","Federated learning (FL) provides a promising framework for enabling distributed machine learning based services without revealing users’ private data. In the scenario of wireless FL, to counter the eavesdropping attack when the parameter-server (PS, which is co-located with a base station, BS) sends the model-data to the wireless devices, we propose a secrecy driven FL via cooperative jamming, in which wireless devices cooperatively provide jamming to the eavesdropper to enhance the PS’s secure throughput based on the measure of physical layer security. We formulate a joint optimization of the PS’s downloading-transmission duration, all wireless devices’ uploading-transmission duration as a non-orthogonal multiple access cluster, each device’s local processing-rate and transmit powers for its uploading NOMA-transmission and jamming to the eavesdropper, with the objective of minimizing the overall latency for each round of FL iteration. Despite the non-convexity of the joint optimization problem, a layered algorithm is proposed to solve it. Taking into account the special feature of the optimal jamming solution, we further propose a benefit-sharing scheme, which is based on the principle of Nash bargaining solution, such that all wireless devices can benefit from reducing the FL latency via cooperative jamming in a fairness manner. Numerical results are provided to validate the effectiveness of our proposed algorithms as well as the performance advantage of our proposed secrecy driven FL via cooperative jamming. Experimental results based on the real data-sets and training models demonstrate that our scheme can reduce the latency by more than 35% compared to the case without using the jamming.","2168-6750","","10.1109/TETC.2022.3159282","Fundo para o Desenvolvimento das Ciências e da Tecnologia(grant numbers:0060/2019/A1,0162/2019/A3); National Research Foundation Singapore; Info-communications Media Development Authority; Future Communications Research & Development Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9738843","Federated learning;physical layer security;secrecy throughput;cooperative jamming;non-orthogonal multiple access","Jamming;Wireless communication;Communication system security;Training;Throughput;Optimization;Computational modeling","cooperative communication;jamming;learning (artificial intelligence);minimisation;radio networks;telecommunication security;wireless channels","base station;downloading-transmission duration;federated learning;FL iteration;FL latency;joint optimization problem;latency minimization;model-data;NOMA-transmission;nonorthogonal multiple access cluster;optimal jamming solution;physical layer security;secure throughput;uploading-transmission duration;wireless devices;wireless FL","","9","","40","IEEE","21 Mar 2022","","","IEEE","IEEE Journals"
"G-VCFL: Grouped Verifiable Chained Privacy-Preserving Federated Learning","Z. Zhang; L. Wu; D. He; Q. Wang; D. Wu; X. Shi; C. Ma","Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; School of Computer Science, University of Windsor, Windsor, Canada; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China","IEEE Transactions on Network and Service Management","1 Feb 2023","2022","19","4","4219","4231","Federated learning, as a typical distributed learning paradigm, shows great potential in Industrial Internet of Things, Smart Home, Smart City, etc. It enables collaborative learning without data leaving local users. Despite the huge benefits, it still faces the risk of privacy breaches and a single point of failure for aggregation server. Adversaries can use intermediate models to infer user privacy, or even return incorrect global model by manipulating the aggregation server. To address these issues, several federated learning solutions focusing on privacy-preserving and security have been proposed. However, theses solutions still faces challenges in resource-limited scenarios. In this paper, we propose G-VCFL, a grouped verifiable chained privacy-preserving federated learning scheme. Specifically, we first use the grouped chain learning mechanism to guarantee the privacy of users, and then propose a verifiable secure aggregation protocol to guarantee the verifiability of the global model. G-VCFL does not require any complex cryptographic primitives and does not introduce noise, but enables verifiable privacy-preserving federated learning by utilizing lightweight pseudorandom generators. We conduct extensive experiments on real-world datasets by comparing G-VCFL with other state-of-the-art approaches. The experimental results and functional evaluation indicate that G-VCFL is efficient in the six experimental cases and satisfies all the intended design goals.","1932-4537","","10.1109/TNSM.2022.3196404","National Key Research and Development Program of China(grant numbers:2021YFB3101104); National Natural Science Foundation of China(grant numbers:U20A20177,61772377); Key R&D plan of Hubei Province(grant numbers:2021BAA025); Open Research Fund from Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)(grant numbers:GML-KF-22-07); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9849844","Federated learning;privacy-preserving;security;verifiable;lightweight","Servers;Collaborative work;Training;Privacy;Computational modeling;Machine learning;Faces","data privacy;groupware;learning (artificial intelligence)","aggregation server;collaborative learning;cryptographic primitives;G-VCFL;grouped chain;grouped verifiable chained privacy-preserving federated learning;Industrial Internet of Things;privacy breaches;smart city;smart home;user privacy","","","","46","IEEE","4 Aug 2022","","","IEEE","IEEE Journals"
"Operating Reserve Quantification Using Prediction Intervals of Wind Power: An Integrated Probabilistic Forecasting and Decision Methodology","C. Zhao; C. Wan; Y. Song","College of Electrical Engineering, Zhejiang University, Hangzhou, China; College of Electrical Engineering, Zhejiang University, Hangzhou, China; State Key Laboratory of Internet of Things for Smart City, University of Macau, Taipa, Macau SAR, China","IEEE Transactions on Power Systems","18 Jun 2021","2021","36","4","3701","3714","Adequate reserves are urgently needed to hedge against wind power forecasting uncertainties in power systems. Traditional reserve quantification sequentially acquires statistical features of wind power and then determines reserve amounts. This paper establishes a novel integrated probabilistic forecasting and decision (IPFD) methodology to simultaneously optimize the wind power prediction intervals (PIs) and probabilistic reserve quantification. Upward and downward reserve quantities are defined to cover the wind power forecasting uncertainties within the PIs. A cost function evaluating the reserve provision payment and deficit penalty is elaborated to realize cost-benefit trade-offs of reserve decision. Nonparametric wind power PIs are constructed based on extreme learning machine, which minimizes the reserve cost function subject to eligible target coverage probability constraint. The confidence level and quantile proportions associated with wind power PIs can be jointly tuned to reduce the operational cost of reserves. Benefited from extreme learning machine, the IPFD model is reformulated as a mixed integer linear programming problem. A feasible region tightening strategy that shrinks the large constant coefficients and eliminates the redundant binary variables is proposed to accelerate model training. Numerical experiments based on actual wind power data demonstrate the remarkable cost-effective advantages of the IPFD based reserve quantification, as well as the high computational efficiency for online application.","1558-0679","","10.1109/TPWRS.2021.3053847","National Key R&D Program of China(grant numbers:2018YFB0905000); National Natural Science Foundation of China(grant numbers:51877189,51761135015,U2066601); Young Elite Scientists Sponsorship Program by China Association of Science and Technology(grant numbers:2018QNRC001); Zhejiang University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9334448","Operating reserve;prediction interval;probabilistic forecasting;wind power;machine learning;mixed integer linear programming","Wind power generation;Machine learning;Probabilistic logic;Uncertainty;Stochastic processes;Wind forecasting;Mixed integer linear programming","cost reduction;cost-benefit analysis;feedforward neural nets;integer programming;numerical analysis;power engineering computing;power generation economics;power generation planning;probability;wind power;wind power plants","wind power prediction intervals;wind power forecasting uncertainties;power systems;traditional reserve quantification;probabilistic reserve quantification;downward reserve quantities;reserve provision payment;deficit penalty;cost-benefit trade-offs;reserve decision;extreme learning machine;actual wind power data;IPFD based reserve quantification;reserve cost function;nonparametric wind power PI;upward reserve quantities;integrated probabilistic forecasting and decision methodology;statistical features;optimization method;target coverage probability constraint;operational cost reduction;mixed integer linear programming problem;large constant coefficients;redundant binary variables;numerical analysis","","29","","43","IEEE","22 Jan 2021","","","IEEE","IEEE Journals"
"Unsupervised Feature Learning Architecture With Multi-Clustering Integration RBM","J. Chu; H. Wang; J. Liu; Z. Gong; T. Li","Institute of Artificial Intelligence, School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China; Institute of Artificial Intelligence, School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China; School of Business, Sichuan University, Chengdu, Sichuan, China; Department of Computer and Information Science, State Key Laboratory of Internet of Things for Smart City, University of Macau, Macau, China; Institute of Artificial Intelligence, School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China","IEEE Transactions on Knowledge and Data Engineering","29 Apr 2022","2022","34","6","3002","3015","In this paper, we present a novel unsupervised feature learning architecture, which consists of a multi-clustering integration module and a variant of RBM termed multi-clustering integration RBM (MIRBM). In the multi-clustering integration module, we apply three clusterers (K-means, affinity propagation and spectral clustering algorithms) to obtain three different clustering partitions (CPs) without any background knowledge or label. Then, an unanimous voting strategy is used to generate a local clustering partition (LCP). The novel MIRBM model is a core feature encoding part of the proposed unsupervised feature learning architecture. The novelty of it is that the LCP as an unsupervised guidance is integrated into one step contrastive divergence (${\mathtt{{CD}}}_{1}$CD1) learning to guide the distribution of the hidden layer features. For the instance in the same LCP cluster, the hidden and reconstructed hidden layer features of the MIRBM model in the proposed architecture tend to constrict together in the training process. Meanwhile, each LCP center tends to disperse from each other as much as possible in the hidden and reconstructed hidden layer during training. The experiments demonstrate that the proposed unsupervised feature learning architecture has more powerful feature representation and generalization capability than the state-of-the-art models for clustering tasks in the Microsoft Research Asia Multimedia (MSRA-MM)2.0 dataset.","1558-2191","","10.1109/TKDE.2020.3015959","National Key R&D Program of China(grant numbers:2019YFB2101802); National Science Foundation of China(grant numbers:61773324,61876158,61976247,71901158); Sichuan Key R&D project(grant numbers:2020YFG0035); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9165942","Multi-clustering integration RBM;unsupervised feature learning; ${\mathtt{{CD}}}_{1}$    CD 1      learning;image clustering","Image reconstruction;Training;Feature extraction;Task analysis;Clustering algorithms;Machine learning;Encoding","generalisation (artificial intelligence);pattern clustering;unsupervised learning","multiclustering integration RBM;multiclustering integration module;spectral clustering algorithms;clustering partition;hidden layer features;LCP cluster;feature representation;generalization capability;unsupervised feature learning architecture;one step contrastive divergence learning;Microsoft Research Asia Multimedia dataset;MSRA-MM2.0 dataset;K-means clustering;affinity propagation clustering algorithm;voting strategy","","9","","54","IEEE","12 Aug 2020","","","IEEE","IEEE Journals"
"Cost-Oriented Prediction Intervals: On Bridging the Gap Between Forecasting and Decision","C. Zhao; C. Wan; Y. Song","College of Electrical Engineering, Zhejiang University, Hangzhou, China; College of Electrical Engineering, Zhejiang University, Hangzhou, China; State Key Laboratory of Internet of Things for Smart City, University of Macau, Taipa, Macau SAR, China","IEEE Transactions on Power Systems","17 Jun 2022","2022","37","4","3048","3062","As an efficient tool for uncertainty quantification of renewable energy forecasting, prediction intervals (PIs) provide essential prognosis to power system operator. Merely improving the statistical quality of PIs with respect to calibration and sharpness cannot always contribute to the operational value for specific decision-making issue. In this paper, the cost-oriented prediction intervals are firstly proposed to achieve the joint improvement of forecasting quality and decision performance. In order to bridge the gap between forecasting and decision, a novel cost-oriented machine learning (COML) framework is established, which unifies nonparametric renewable power PI construction and decision-making. Formulated as a bilevel programming model, the COML minimizes the operational costs of decision-making process by adaptively adjusting the quantile proportion pair of PIs resulting from extreme learning machine based quantile regression. The hierarchical optimization model of the COML is equivalently simplified as a single level nonlinear programming problem. Then an enhanced branch-and-contract algorithm with innovative bounds contraction strategy is devised to efficiently capture the optimum of the single level problem with bilinear nonconvexity. Numerical experiments based on actual wind farm data simulate the online forecasting and decision process for wind power offering. Comprehensive comparisons verify the substantial superiority of the proposed COML methodology in terms of forecasting quality, operational value, as well as computational efficiency for practical application.","1558-0679","","10.1109/TPWRS.2021.3128567","National Key R&D Program of China(grant numbers:2018YFB0905000); National Natural Science Foundation of China(grant numbers:51877189,U2166203,U2066601); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LR22E070003); China Association of Science and Technology(grant numbers:2018QNRC001); Zhejiang University Academic Award; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9616458","Prediction interval;forecasting;renewable energy;decision making;uncertainty;machine learning","Forecasting;Renewable energy sources;Probabilistic logic;Uncertainty;Wind power generation;Decision making;Costs","cost reduction;decision making;learning (artificial intelligence);minimisation;nonlinear programming;power system economics;power system planning;regression analysis;renewable energy sources","COML;operational costs;extreme learning machine;single level nonlinear programming problem;operational value;cost-oriented prediction intervals;renewable energy forecasting;statistical quality;specific decision-making issue;decision performance;nonparametric renewable power PI construction;uncertainty quantification;forecasting quality;cost-oriented machine learning framework;quantile regression;hierarchical optimization model;enhanced branch-and-contract algorithm;innovative bounds contraction strategy;single level problem;bilinear nonconvexity;operational costs minimization","","10","","48","IEEE","16 Nov 2021","","","IEEE","IEEE Journals"
"Enabling Trustworthiness in Sustainable Energy Infrastructure Through Blockchain and AI-Assisted Solutions","S. Otoum; H. T. Mouftah","Zayed University, United Arab Emirates; University of Ottawa, Canada","IEEE Wireless Communications","21 Jan 2022","2021","28","6","19","25","Network trustworthiness is a critical component of network security, as it builds on positive inter-actions, guarantees, transparency, and accountability. And with the growth of smart city services and applications, trustworthiness is becoming more important. Most current network trustworthiness solutions are insufficient, particularly for critical infrastructures where end devices are vulnerable and easily hacked. In terms of the energy sector, blockchain technology transforms all currencies into digital modes, thereby allowing one person to manage and exchange energy with others. This has drawn the attention of experts in many fields as a safe, low-cost platform to track billions of transactions in a distributed energy economy. Security and trust issues are still relatively new in the current centralized energy management scheme. With blockchain technology, a decentralized energy infrastructure enables parties to establish micro- grid trading energy transactions and apply artificial intelligence (AI). Using AI in energy systems enables machines to learn various parameters, such as predicted required amounts, excess amounts, and trusted partners. In this article, we envision a cooperative and distributed framework based on cutting-edge computing, communication, and intelligence capabilities such as AI and blockchain in the energy sector to enable secure energy trading, remote monitoring, and trustworthiness. The proposed framework can also enable secure energy trading at the edge devices and among multiple devices. There are also discussions on difficulties, issues, and design principles, as well as spotlights on some of the more popular solutions.","1558-0687","","10.1109/MWC.018.2100194","Zayed University(grant numbers:R20130); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9690147","","Training;Smart cities;Transforms;Machine learning;Network security;Blockchains;Artificial intelligence","artificial intelligence;computer network security;distributed power generation;energy management systems;telecommunication power management;trusted computing","smart city services;current network trustworthiness solutions;critical infrastructures;energy sector;technology transforms all currencies;low-cost platform;distributed energy economy;trust issues;current centralized energy management scheme;blockchain technology;decentralized energy infrastructure;micro grid trading energy transactions;AI;energy systems;secure energy trading;popular solutions;enabling trustworthiness;sustainable energy infrastructure;critical component;network security;positive inter-actions","","3","","15","IEEE","21 Jan 2022","","","IEEE","IEEE Magazines"
"A Blockchain-Based Architecture and Framework for Cybersecure Smart Cities","A. E. Bekkali; M. Essaaidi; M. Boulmalf","Ecole Nationale Supérieure d’Informatique et d’Analyse des Systèmes (ENSIAS), Mohammed V University, Rabat, Morocco; Ecole Nationale Supérieure d’Informatique et d’Analyse des Systèmes (ENSIAS), Mohammed V University, Rabat, Morocco; International University of Rabat, Rabat, Morocco","IEEE Access","28 Jul 2023","2023","11","","76359","76370","A smart city is one that uses digital technologies and other means to improve the quality of life of its citizens and reduce the cost of municipal services. Smart cities primarily use IoT to collect and analyze data to interact directly with the city’s infrastructure and monitor city assets and community developments in real time to improve operational efficiency and proactively respond to potential problems and challenges. Today, cybersecurity is considered one of the main challenges facing smart cities. Over the past few years, the cybersecurity research community has devoted a great deal of attention to this challenge. Among the various technologies being considered to meet this challenge, Blockchain is emerging as a solution offering the data security and confidentiality essential for strengthening the security of smart cities. In this paper, we propose a comprehensive framework and architecture based on Blockchain, big data and artificial intelligence to improve smart cities cybersecurity. To illustrate the proposed framework in detail, we present simulation results accompanied by analyses and tests. These simulations were carried out on a smart grid dataset from the UCI Machine Learning Repository. The results convincingly demonstrate the potential and effectiveness of the proposed framework for addressing cybersecurity challenges in smart cities. These results reinforce the relevance and applicability of the framework in a real-world context.","2169-3536","","10.1109/ACCESS.2023.3296482","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10185202","Smart city;smart grid;cybersecurity;framework;IoT;blockchain;big data;artificial intelligence","Blockchains;Smart cities;Internet of Things;Security;Computer security;Consensus protocol;Big Data","Big Data;blockchains;Internet of Things;learning (artificial intelligence);smart cities","artificial intelligence;big data;blockchain-based architecture;city assets;cybersecure smart cities;data security;digital technology;IoT;municipal services;smart cities cybersecurity;smart grid dataset;UCI machine learning repository","","","","45","CCBYNCND","18 Jul 2023","","","IEEE","IEEE Journals"
"MECC: A Mobile Edge Collaborative Caching Framework Empowered by Deep Reinforcement Learning","S. Xu; X. Liu; S. Guo; X. Qiu; L. Meng",Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications,"IEEE Network","20 Aug 2021","2021","35","4","176","183","With the rapid development of smart city and 5G, user demand for Internet services has increased exponentially. Through collaborative content sharing, the storage limitation of a single edge server (ES) can be broken. However, when mobile users need to download the whole content through multiple regions, independently deciding the caching content for ESs in different regions may result in redundant caching. Furthermore, frequent switching of communication connection during user movement also causes retransmission delay. As a revolutionary approach in the artificial intelligence field, deep reinforcement learning (DRL) has earned great success in solving high-dimensional and network resource management related problems. Therefore, we integrate collaborative caching and DRL to build an intelligent edge caching framework, so as to realize collaborative caching between cloud and ESs. In this caching framework, a fed-erated-machine-learning-based user behavior prediction model is first designed to characterize the content preference and movement trajectory of mobile users. Next, to achieve efficient resource aggregation of ESs, a user-behavior-aware dynamic collaborative caching domain (DCCD) construction and management mechanism is devised to divide ESs into clusters, select cluster heads, and set the re-clustering rules. Then a DRL-based content caching and delivery algorithm is presented to decide the caching content of ESs in a DCCD from a global perspective and plan the transmission path for users, which reduces redundant content and transmission delay. Especially when a user request cannot be satisfied by the current DCCD, a cross-domain content delivery strategy is presented to allow ESs in other DCCDs to provide and forward content to the user, avoiding the traffic pressure and delay caused by requesting services from cloud. The simulation results show that the proposed collaborative caching framework can improve user satisfaction in terms of content hit rate and service delay.","1558-156X","","10.1109/MNET.011.2000663","National Natural Science Foundation of China(grant numbers:62071070); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9520343","","Smart cities;Simulation;Web and internet services;Collaboration;Reinforcement learning;Switches;Predictive models","5G mobile communication;deep learning (artificial intelligence);mobile computing;telecommunication network management;telecommunication traffic","deep reinforcement learning;collaborative content sharing;single edge server;mobile users;caching content;redundant caching;user movement;artificial intelligence field;DRL;high-dimensional network resource management related problems;intelligent edge caching framework;fed-erated-machine-learning-based user behavior prediction model;content preference;movement trajectory;management mechanism;delivery algorithm;redundant content;user request;cross-domain content delivery strategy;user satisfaction;content hit rate;service delay;mobile edge collaborative caching framework","","4","","15","IEEE","20 Aug 2021","","","IEEE","IEEE Magazines"
"Graph-Enabled Intelligent Vehicular Network Data Processing","Z. Zheng; A. K. Bashir","School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Computing and Mathematics, Manchester Metropolitan University, Manchester, U.K","IEEE Transactions on Intelligent Transportation Systems","3 May 2022","2022","23","5","4726","4735","Intelligent vehicular network (IVN) is the underlying support for the connected vehicles and smart city, but there are several challenges for IVN data processing due to the dynamic structure of the vehicular network. Graph processing, as one of the essential machine learning and big data processing paradigm, which provide a set of big data processing scheme, is well-designed to processing the connected data. In this paper, we discussed the research challenges of IVN data processing and motivated us to address these challenges by using graph processing technologies. We explored the characteristics of the widely used graph algorithms and graph processing frameworks on GPU. Furthermore, we proposed several graph-based optimization technologies for IVN data processing. The experimental results show the graph processing technologies on GPU can archive excellent performance on IVN data.","1558-0016","","10.1109/TITS.2022.3158045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9750064","Graph processing;vehicular networks;internet of intelligent vehicles","Data processing;Quality of service;Vehicle dynamics;Sensors;Path planning;Job shop scheduling;Industries","Big Data;graph theory;graphics processing units;learning (artificial intelligence);mobile computing;optimisation;vehicular ad hoc networks","IVN data processing;graph-based optimization technologies;graph-enabled intelligent vehicular network data processing;GPU;machine learning;Big Data processing scheme;connected vehicles;smart city","","5","","55","IEEE","5 Apr 2022","","","IEEE","IEEE Journals"
"A Game Theoretic Analysis for Power Management and Cost Optimization of Green Base Stations in 5G and Beyond Communication Networks","P. Gorla; A. Deshmukh; S. Joshi; V. Chamola; M. Guizani","Department of Electrical and Electronics Engineering, Birla Institute of Technology and Science Pilani, Pilani, India; Qualcomm India Pvt. Ltd., Hyderabad, India; Department of Electrical and Electronics Engineering, Birla Institute of Technology and Science Pilani, Pilani, India; Department of Electrical and Electronics Engineering, Birla Institute of Technology and Science Pilani, Pilani, India; Machine Learning Department, Mohamed Bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE","IEEE Transactions on Network and Service Management","12 Oct 2022","2022","19","3","2714","2725","Due to the exponential increase in the number of users, the next-generation cellular networks are resource-constrained in power and bandwidth. Power consumption is one of the critical consideration for the next-generation wireless networks, therefore, management of available resources is essential to achieve power efficiency. With the growing incentive to ‘go green’ and to reduce the carbon footprint, the fifth generation (5G) and beyond wireless networks will derive power from renewable sources to solve the energy efficiency problems. This work focuses on integrated regulation of the traditional, i.e., the grid-based and the renewable, i.e., the solar-based power supplies for the 5G and beyond 5G green base stations (BSs) in a smart city scenario. We propose a pricing model for suppliers to charge the BSs for electricity consumption when the renewable power supply cannot meet their total energy requirements. We propose a game-theoretic analysis for cost optimization by proposing two games, i.e., the power control game and the best supplier game. Each BS acts as a game player and has some actions like power reduction and supplier selection to reduce the total energy costs. We also provide the game transition profiles for the BSs. Furthermore, the Nash Equilibrium’s existence is verified for each of these games and an optimal cost solution is proposed for the green BSs.","1932-4537","","10.1109/TNSM.2022.3149593","DST-SERB [Science and Engineering Research Board (SERB)](grant numbers:ECR/2018/0001479); Science and Engineering Research Board (SERB), Government of India, through its Start-up Research Grant (SRG)(grant numbers:SRG/2021/000175); NVIDIA Corporation with the donation of the Quadro P5000 GPU to Dr. Vinay which was used for this research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9706323","5G;game theory;green communication;Nash Equilibrium;resource management","Games;Costs;Batteries;Power demand;5G mobile communication;Renewable energy sources;Cellular networks","","","","3","","45","IEEE","7 Feb 2022","","","IEEE","IEEE Journals"
"An End-to-End Trainable Feature Selection-Forecasting Architecture Targeted at the Internet of Things","M. Nakip; K. Karakayali; C. Güzelı̇ş; V. Rodoplu","Polish Academy of Sciences (PAN), Institute of Theoretical and Applied Informatics, Gliwice, Poland; ETECube, Izmir Technology Development Zone, Izmir Institute of Technology, Izmir, Turkey; Department of Electrical and Electronics Engineering, Yaşar University, Izmir, Turkey; Department of Electrical and Electronics Engineering, Yaşar University, Izmir, Turkey","IEEE Access","29 Jul 2021","2021","9","","104011","104028","We develop a novel end-to-end trainable feature selection-forecasting (FSF) architecture for predictive networks targeted at the Internet of Things (IoT). In contrast with the existing filter-based, wrapper-based and embedded feature selection methods, our architecture enables the automatic selection of features dynamically based on feature importance score calculation and gamma-gated feature selection units that are trained jointly and end-to-end with the forecaster. We compare the performance of our FSF architecture on the problem of forecasting IoT device traffic against the following existing (feature selection, forecasting) technique pairs: Autocorrelation Function (ACF), Analysis of Variance (ANOVA), Recurrent Feature Elimination (RFE) and Ridge Regression methods for feature selection, and Linear Regression, Multi-Layer Perceptron (MLP), Long Short Term Memory (LSTM), 1 Dimensional Convolutional Neural Network (1D CNN), Autoregressive Integrated Moving Average (ARIMA), and Logistic Regression for forecasting. We show that our FSF architecture achieves either the best or close to the best performance among all of the competing techniques by virtue of its dynamic, automatic feature selection capability. In addition, we demonstrate that both the training time and the execution time of FSF are reasonable for IoT applications. This work represents a milestone for the development of predictive networks for IoT in smart cities of the near future.","2169-3536","","10.1109/ACCESS.2021.3092228","European Union’s Horizon 2020 Research and Innovation Program under the Marie Skłodowska-Curie(grant numbers:846077); “Quality of Service for the Internet of Things in Smart Cities via Predictive Networks”; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9477183","Forecasting;feature selection;machine learning;neural network;Internet of Things (IoT);predictive network;smart city","Forecasting;Feature extraction;Computer architecture;Internet of Things;Smart cities;Training;Performance evaluation","autoregressive moving average processes;feature extraction;feature selection;forecasting theory;Internet of Things;multilayer perceptrons;regression analysis;support vector machines","Recurrent Feature Elimination;FSF architecture;dynamic feature selection capability;automatic feature selection capability;novel end-to-end trainable feature selection-forecasting architecture;feature selection methods;feature importance score calculation;gamma-gated feature selection units;forecaster;Internet of Things;autocorrelation function;analysis of variance;ridge regression methods;linear regression;multi-layer perceptron;1 Dimensional convolutional neural network;autoregressive integrated moving average;logistic regression","","7","","42","CCBYNCND","7 Jul 2021","","","IEEE","IEEE Journals"
"Distributed Variational Bayes-Based In-Network Security for the Internet of Things","W. He; Y. Liu; H. Yao; T. Mai; N. Zhang; F. R. Yu","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Department of of the National Engineering, Laboratory for Risk Perception and Prevention, China Academy of Electronics and Information Technology, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; China Electronic Corporation, Sixth Research Institute, Beijing, China; Department of Systems Computer Engineering, Carleton University, Ottawa, Canada","IEEE Internet of Things Journal","7 Apr 2021","2021","8","8","6293","6304","The past few years have witnessed the compelling applications of the Internet of Things (IoT) in our daily life. The explosive growth of the number of IoT devices also presents a great challenge in network security, especially the DDoS attack. Current DDoS defense mechanisms adopted out-of-band architecture, which is accomplished by a process that receives monitoring data from routers and switches, then analyzes that flow data to detect attacks. However, facing IoT devices growing rapidly, this out-of-band architecture confronted with limited processing capacity, bandwidth resources, and service assurance problems. Recently, with the development of the programming switch, it opens up new possibilities for in-network DDoS detection, where the detection algorithms could be directly implemented inside the routers and switches. Benefit from switch processing performance, the in-network mechanism could achieve high scalability and line speed performance. Therefore, in this article, we design a machine learning-based in-network DDoS detection framework. We implement the lightweight variational Bayes algorithm in each switch to detect the anomaly traffic. Besides, considering the shortage of training data in each switch, a centralized platform is introduced to synchronize parameters among distributed switches to realize collaborative learning. Extensive simulations are conducted to evaluate our proposed algorithm in comparison to some state-of-the-art schemes.","2327-4662","","10.1109/JIOT.2020.3041656","New Generation of Artificial Intelligence Special Action Project(grant numbers:AI20191125008); Artificial Intelligence and Smart City Joint Laboratory (BUPT-TGSTII)(grant numbers:B2020001); Future Intelligent Networking and Intelligent Transportation Joint Laboratory (BUPT-CTTIC)(grant numbers:B2019007); Open Research Fund of State Key Laboratory of Space-Ground Integrated Information Technology(grant numbers:2018_SGIIT_KFJJ_TX_03); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9274475","DDoS;distributed variational Bayes;in-network security;Internet of Things (IoT)","Computer crime;Security;Internet of Things;Denial-of-service attack;Monitoring;Performance evaluation;Optimization","Bayes methods;computer network security;Internet of Things;learning (artificial intelligence);resource allocation;telecommunication network routing;telecommunication switching;telecommunication traffic;variational techniques","machine learning-based in-network DDoS detection framework;Internet of Things;distributed variational Bayes-based in-network security;distributed switches;lightweight variational Bayes algorithm;line speed performance;in-network mechanism;switch processing performance;out-of-band architecture;DDoS defense mechanisms;DDoS attack;IoT devices","","5","","28","IEEE","1 Dec 2020","","","IEEE","IEEE Journals"
"Harnessing the Power of Smart and Connected Health to Tackle COVID-19: IoT, AI, Robotics, and Blockchain for a Better World","F. Firouzi; B. Farahani; M. Daneshmand; K. Grise; J. Song; R. Saracco; L. L. Wang; K. Lo; P. Angelov; E. Soares; P. -S. Loh; Z. Talebpour; R. Moradi; M. Goodarzi; H. Ashraf; M. Talebpour; A. Talebpour; L. Romeo; R. Das; H. Heidari; D. Pasquale; J. Moody; C. Woods; E. S. Huang; P. Barnaghi; M. Sarrafzadeh; R. Li; K. L. Beck; O. Isayev; N. Sung; A. Luo","Electrical and Computer Engineering Department, Duke University, Durham, NC, USA; Cyberspace Research Institute, Shahid Beheshti University, Tehran, Iran; Business Intelligence and Analytics, Stevens Institute of Technology, Hoboken, NJ, USA; IEEE Future Directions, Piscataway, NJ, USA; Department of Computer and Information Security, Sejong University, Seoul, South Korea; IEEE Future Directions, Piscataway, NJ, USA; Allen Institute for Artificial Intelligence, Seattle, WA, USA; Allen Institute for Artificial Intelligence, Seattle, WA, USA; School of Computing and Communications, Lancaster University, Lancashire, U.K.; School of Computing and Communications, Lancaster University, Lancashire, U.K.; Department of Mathematical Sciences, Carnegie Mellon University, Pittsburgh, PA, USA; Cyberspace Research Institute, Shahid Beheshti University, Tehran, Iran; Cyberspace Research Institute, Shahid Beheshti University, Tehran, Iran; Cyberspace Research Institute, Shahid Beheshti University, Tehran, Iran; Sina Hospital, Tehran, Iran; Sina Hospital, Tehran, Iran; Cyberspace Research Institute, Shahid Beheshti University, Tehran, Iran; Department of Information Engineering, Universit Politecnica delle Marche, Ancona, Italy; James Watt School of Engineering, University of Glasgow, Glasgow, U.K.; James Watt School of Engineering, University of Glasgow, Glasgow, U.K.; School of Medicine and Duke Health, Duke University, Durham, NC, USA; School of Medicine and Duke Health, Duke University, Durham, NC, USA; School of Medicine and Duke Health, Duke University, Durham, NC, USA; School of Medicine and Duke Health, Duke University, Durham, NC, USA; Department of Brain Sciences, Imperial College London, London, U.K.; Computer Science Department & Electrical and Computer Engineering Department, University of California at Los Angeles, Los Angeles, CA, USA; Department of Medicine, Stanford University School of Medicine, Stanford, CA, USA; Almaden Research Center, IBM, San Jose, CA, USA; Department of Chemistry, Carnegie Mellon University, Pittsburgh, PA, USA; Korea Electronics Technology Institute, Seongnam, South Korea; Computer Science Department, Stanford University, Stanford, CA, USA","IEEE Internet of Things Journal","5 Aug 2021","2021","8","16","12826","12846","As COVID-19 hounds the world, the common cause of finding a swift solution to manage the pandemic has brought together researchers, institutions, governments, and society at large. The Internet of Things (IoT), artificial intelligence (AI)—including machine learning (ML) and Big Data analytics—as well as Robotics and Blockchain, are the four decisive areas of technological innovation that have been ingenuity harnessed to fight this pandemic and future ones. While these highly interrelated smart and connected health technologies cannot resolve the pandemic overnight and may not be the only answer to the crisis, they can provide greater insight into the disease and support frontline efforts to prevent and control the pandemic. This article provides a blend of discussions on the contribution of these digital technologies, propose several complementary and multidisciplinary techniques to combat COVID-19, offer opportunities for more holistic studies, and accelerate knowledge acquisition and scientific discoveries in pandemic research. First, four areas, where IoT can contribute are discussed, namely: 1) tracking and tracing; 2) remote patient monitoring (RPM) by wearable IoT (WIoT); 3) personal digital twins (PDTs); and 4) real-life use case: ICT/IoT solution in South Korea. Second, the role and novel applications of AI are explained, namely: 1) diagnosis and prognosis; 2) risk prediction; 3) vaccine and drug development; 4) research data set; 5) early warnings and alerts; 6) social control and fake news detection; and 7) communication and chatbot. Third, the main uses of robotics and drone technology are analyzed, including: 1) crowd surveillance; 2) public announcements; 3) screening and diagnosis; and 4) essential supply delivery. Finally, we discuss how distributed ledger technologies (DLTs), of which blockchain is a common example, can be combined with other technologies for tackling COVID-19.","2327-4662","","10.1109/JIOT.2021.3073904","Smart City Research and Development Project of the Korea Agency for Infrastructure Technology Advancement (KAIA) Grant; Ministry of Land, Infrastructure and Transport (MOLIT), Ministry of Science and ICT (MSIT)(grant numbers:18NSPS-B149388-01); Vision Robotics Artificial Intelligence Group; RISC-19 ICU Board; Bioengineering Lab Group; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9406879","Artificial intelligence (AI);big data;blockchain;COVID-19;digital twin;eHealth;healthcare;Internet of Things (IoT);pandemic;robotics;wearable","COVID-19;Pandemics;Artificial intelligence;Medical services;Internet of Things;Vaccines;Robots","Big Data;blockchains;data analysis;diseases;epidemics;health care;innovation management;Internet of Things;knowledge acquisition;learning (artificial intelligence);medical robotics;microorganisms;patient monitoring;technology management","COVID-19;AI;robotics;blockchain;technological innovation;connected health technology;disease;digital technologies;pandemic research;wearable IoT;drone technology;distributed ledger technologies;smart health technology;Internet of Things;artificial intelligence;machine learning;Big Data analytics;knowledge acquisition;scientific discoveries","","47","","74","IEEE","19 Apr 2021","","","IEEE","IEEE Journals"
"Vehicle Detection and Tracking in Adverse Weather Using a Deep Learning Framework","M. Hassaballah; M. A. Kenk; K. Muhammad; S. Minaee","Department of Computer Science, Faculty of Computers and Information, South Valley University, Qena, Egypt; Department of Mathematics, Faculty of Science, South Valley University, Qena, Egypt; Department of Software, Sejong University, Seoul, South Korea; Snap Inc., Santa Monica, CA, USA","IEEE Transactions on Intelligent Transportation Systems","12 Jul 2021","2021","22","7","4230","4242","Vehicle detection and tracking play an important role in autonomous vehicles and intelligent transportation systems. Adverse weather conditions such as the presence of heavy snow, fog, rain, dust or sandstorm situations are dangerous restrictions on camera’s function by reducing visibility, affecting driving safety. Indeed, these restrictions impact the performance of detection and tracking algorithms utilized in the traffic surveillance systems and autonomous driving applications. In this article, we start by proposing a visibility enhancement scheme consisting of three stages: illumination enhancement, reflection component enhancement, and linear weighted fusion to improve the performance. Then, we introduce a robust vehicle detection and tracking approach using a multi-scale deep convolution neural network. The conventional Gaussian mixture probability hypothesis density filter based tracker is utilized jointly with hierarchical data associations (HDA), which splits into detection-to-track and track-to-track associations. Herein, the cost matrix of each phase is solved using the Hungarian algorithm to compensate for the lost tracks caused by missed detection. Only detection information (i.e., bounding boxes with detection scores) is used in HDA without visual features information for rapid execution. We have also introduced a novel benchmarking dataset designed for research in applications of autonomous vehicles under adverse weather conditions called DAWN. It consists of real-world images collected with different types of adverse weather conditions. The proposed method is tested on DAWN, KITTI, and MS-COCO datasets and compared with 21 vehicle detectors. Experimental results have validated effectiveness of the proposed method which outperforms state-of-the-art vehicle detection and tracking approaches under adverse weather conditions.","1558-0016","","10.1109/TITS.2020.3014013","Institute of Information and communications Technology Planning and Evaluation (IITP); Korean Government (MSIT), Development of AI-Convergence Technologies for Smart City Industry Productivity Innovation(grant numbers:2019-0-00136); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9184996","Object detection;vehicles detection/tracking;deep learning models;intelligent transportation systems","Meteorology;Lighting;Detectors;Vehicle detection;Image restoration;Image color analysis;Machine learning","convolutional neural nets;deep learning (artificial intelligence);driver information systems;filtering theory;Gaussian processes;image fusion;object detection;object tracking;probability;road safety;road vehicles;video surveillance","tracking approach;adverse weather conditions;deep learning;autonomous vehicles;intelligent transportation systems;dust;dangerous restrictions;driving safety;traffic surveillance systems;autonomous driving applications;visibility enhancement scheme;illumination enhancement;reflection component enhancement;robust vehicle detection;multiscale deep convolution neural network;detection-to-track associations;track-to-track associations;detection information;detection scores;vehicle detectors;vehicle detection;Gaussian mixture probability hypothesis density filter based tracker","","51","","55","IEEE","2 Sep 2020","","","IEEE","IEEE Journals"
"HarMI: Human Activity Recognition Via Multi-Modality Incremental Learning","X. Zhang; H. Yu; Y. Yang; J. Gu; Y. Li; F. Zhuang; D. Yu; Z. Ren","School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science and Technology, Shandong University, Qingdao, China; Nanjing University of Science and Technology, Nanjing, China; Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Information Science and Engineering, Shandong University, Qingdao, China; Institute of Artificial Intelligence, Beihang University, Beijing, China; School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science and Technology, Shandong University, Qingdao, China","IEEE Journal of Biomedical and Health Informatics","7 Mar 2022","2022","26","3","939","951","Nowadays, with the development of various kinds of sensors in smartphones or wearable devices, human activity recognition (HAR) has been widely researched and has numerous applications in healthcare, smart city, etc. Many techniques based on hand-crafted feature engineering or deep neural network have been proposed for sensor based HAR. However, these existing methods usually recognize activities offline, which means the whole data should be collected before training, occupying large-capacity storage space. Moreover, once the offline model training finished, the trained model can’t recognize new activities unless retraining from the start, thus with a high cost of time and space. In this paper, we propose a multi-modality incremental learning model, called HarMI, with continuous learning ability. The proposed HarMI model can start training quickly with little storage space and easily learn new activities without storing previous training data. In detail, we first adopt attention mechanism to align heterogeneous sensor data with different frequencies. In addition, to overcome catastrophic forgetting in incremental learning, HarMI utilizes the elastic weight consolidation and canonical correlation analysis from a multi-modality perspective. Extensive experiments based on two public datasets demonstrate that HarMI can achieve a superior performance compared with several state-of-the-arts.","2168-2208","","10.1109/JBHI.2021.3085602","National Natural Science Foundation of China(grant numbers:62072279,62006118); Fundamental Research Funds of Shandong University; CCF- Baidu Open Fund(grant numbers:CCF-BAIDU OF2020011); Natural Science Foundation of Jiangsu Province(grant numbers:BK20200460); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9444883","Catastrophic forgetting;incremental learning;human activity recognition;mobile device;multi-modality","Sensors;Training;Data models;Activity recognition;Correlation;Intelligent sensors;Training data","deep learning (artificial intelligence);image motion analysis;smart phones","human activity recognition;wearable devices;smart city;hand-crafted feature engineering;large-capacity storage space;multimodality incremental learning model;continuous learning ability;HarMI model;smartphones;deep neural network","Human Activities;Humans;Machine Learning;Neural Networks, Computer;Smartphone;Wearable Electronic Devices","4","","46","IEEE","1 Jun 2021","","","IEEE","IEEE Journals"
"Deep Learning for Short-Term Prediction of Available Bikes on Bike-Sharing Stations","E. Collini; P. Nesi; G. Pantaleo","Department of Information Engineering, Distributed Systems and Internet Technologies Laboratory, University of Florence, Firenze, Italy; Department of Information Engineering, Distributed Systems and Internet Technologies Laboratory, University of Florence, Firenze, Italy; Department of Information Engineering, Distributed Systems and Internet Technologies Laboratory, University of Florence, Firenze, Italy","IEEE Access","14 Sep 2021","2021","9","","124337","124347","Bike-sharing is adopted as a valid option replacing traditional public transports since they are eco-friendly, prevent traffic congestions, reduce any possible risk of social contacts which happen mostly on public means. However, some problems may occur such as the irregular distribution of bikes on related stations/racks/areas, and the difficulty of knowing in advance what the rack status will be like, or predicting if there will be bikes available in a specific bike-station at a certain time of the day, or if there will be a free slot to leave the rented bike. Thus, providing predictions can be useful to improve the service quality, especially in those cases where bike racks are used for e-bikes, which need to be recharged. This paper compares the state-of-the-art techniques to predict the number of available bikes and free bike-slots in bike-sharing stations (i.e., bike racks). To this end, a set of features and predictive models were compared to identify the best models and predictors for short-term predictions, namely of 15, 30, 45, and 60 minutes. The study has demonstrated that deep learning and in particular Bidirectional Long Short-Term Memory networks (Bi-LSTM) offers a robust approach for the implementation of reliable and fast predictions of available bikes, even with a limited amount of historical data. This paper has also reported an analysis of feature relevance based on SHAP that demonstrated the validity of the model for different cluster behaviours. Both solution and its validation were derived by using data collected in bike-stations in the cities of Siena and Pisa (Italy), in the context of Sii-Mobility National Research Project on Mobility and Transport and Snap4City Smart City IoT infrastructure.","2169-3536","","10.1109/ACCESS.2021.3110794","Ministero dellIstruzione dellUniversit e della Ricerca(grant numbers:Sii-Mobility SCN112); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9530580","Available bikes prediction;bike-sharing;deep learning;machine learning;prediction models;smart city","Radio frequency;Predictive models;Humidity;Deep learning;Wind speed;Biological system modeling;Support vector machines","bicycles;deep learning (artificial intelligence);recurrent neural nets;traffic engineering computing","short-term prediction;bike-sharing stations;public transports;rented bike;bike racks;e-bikes;free bike-slots;predictive models;bidirectional long short-term memory networks;time 15.0 min;time 30.0 min;time 45.0 min;time 60.0 min","","8","","43","CCBY","7 Sep 2021","","","IEEE","IEEE Journals"
"Efficient Algorithms for Kernel Aggregation Queries","T. N. Chan; L. H. U; R. Cheng; M. L. Yiu; S. Mittal","Department of Computer Science, The University of Hong Kong, Hong Kong; Department of Computer and Information Science, State Key Laboratory of Internet of Things for Smart City, University of Macau, Taipa, Macau; Department of Computer Science, The University of Hong Kong, Hong Kong; Department of Computing, Hong Kong Polytechnic Univertiy, Hong Kong; Department of Computer Science, The University of Hong Kong, Hong Kong","IEEE Transactions on Knowledge and Data Engineering","29 Apr 2022","2022","34","6","2726","2739","Kernel functions support a broad range of applications that require tasks like density estimation, classification, regression or outlier detection. For these tasks, a common online operation is to compute the weighted aggregation of kernel function values with respect to a set of points. However, scalable aggregation methods are still unknown for typical kernel functions (e.g., Gaussian kernel, polynomial kernel, sigmoid kernel and additive kernels) and weighting schemes. In this paper, we propose a novel and effective bounding technique, by leveraging index structures, to speed up the computation of kernel aggregation. In addition, we extend our technique to additive kernel functions, including $\chi ^2$χ2, intersection, JS and Hellinger kernels, which are widely used in different communities, e.g., computer vision, medical science, Geoscience etc. To handle the additive kernel functions, we further develop the novel and effective bound functions to efficiently evaluate the kernel aggregation. Experimental studies on many real datasets reveal that our proposed solution KARL achieves at least one order of magnitude speedup over the state-of-the-art for different types of kernel functions.","1558-2191","","10.1109/TKDE.2020.3018376","National Key Research and Development Plan of China(grant numbers:2019YFB2102100); Research Grants Council of Hong Kong(grant numbers:HKU 17229116,106150091,17205115); University of Hong Kong(grant numbers:104004572,102009508,104004129); Innovation and Technology Commission of Hong Kong(grant numbers:ITF project MRP/029/18); Science and Technology Development Fund, Macau SAR(grant numbers:0015/2019/AKP,SKL-IOTSC-2018-2020); University of Macau(grant numbers:MYRG2019-00119-FST); Hong Kong RGC(grant numbers:GRF 152050/19E); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9172120","KARL;Kernel functions;lower and upper bounds","Kernel;Additives;Support vector machines;Libraries;Estimation;Machine learning;Indexes","computer vision;Gaussian processes;pattern classification;polynomials;query processing;support vector machines","kernel aggregation queries;regression;weighted aggregation;kernel function values;scalable aggregation methods;typical kernel functions;Gaussian kernel;polynomial kernel;sigmoid kernel;additive kernels;additive kernel functions;Hellinger kernels;effective bound functions","","2","","58","IEEE","20 Aug 2020","","","IEEE","IEEE Journals"
"Deep-Learning-Driven Proactive Maintenance Management of IoT-Empowered Smart Toilet","E. W. K. See-To; X. Wang; K. -Y. Lee; M. -L. Wong; H. -N. Dai","Department of Computing and Decision Sciences, Lingnan University, Tuen Mun, Hong Kong; Department of Computing and Decision Sciences, Lingnan University, Tuen Mun, Hong Kong; Department of Computing and Decision Sciences, Lingnan University, Tuen Mun, Hong Kong; Department of Computing and Decision Sciences, Lingnan University, Tuen Mun, Hong Kong; Lingnan University, Tuen Mun, Hong Kong","IEEE Internet of Things Journal","23 Jan 2023","2023","10","3","2417","2429","The recent proliferation of Internet of Things (IoT) sensors has driven a myriad of industrial and urban applications. Through analyzing massive data collected by these sensors, the proactive maintenance management can be achieved such that the maintenance schedule of the installed equipment can be optimized. Despite recent progress in proactive maintenance management in industrial scenarios, there are few studies on proactive maintenance management in urban informatics. In this article, we present an integrated framework of IoT and cloud computing platform for the proactive maintenance management in smart city. Our framework consists of: 1) an IoT monitoring system for collecting time-series data of operating and ambient conditions of the equipment and 2) a hybrid deep learning model, namely, convolutional bidirectional long short-term memory (CBLM) model for forecasting the operating and ambient conditions based on the collected time-series data. In addition, we also develop a naïve Bayes classifier to detect abnormal operating and ambient conditions and assist management personnel in scheduling maintenance tasks. To evaluate our framework, we deployed the IoT system in a Hong Kong public toilet, which is the first application of proactive maintenance management for a public hygiene and sanitary facility to the best of our knowledge. We collected the sensed data more than 33 days (808 h) in this real system. Extensive experiments on the collected data demonstrated that our proposed CBLM outperformed six traditional machine learning algorithms.","2327-4662","","10.1109/JIOT.2022.3211889","LEO Dr. David P. Chan Institute of Data Science, Lingnan University; Innovation and Technology Commission—The Government of the Hong Kong Special Administrative Region; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9917480","Cloud computing;convolutional neural network (CNN);deep learning;Internet of Things (IoT);long short-term memory (LSTM);machine learning;proactive maintenance management","Data models;Personnel;Maintenance management;Task analysis;Sensors;Production;Predictive models","cloud computing;computer network security;deep learning (artificial intelligence);Internet of Things;maintenance engineering;pattern classification;recurrent neural nets;smart cities;time series","CBLM;cloud computing platform;convolutional bidirectional long short-term memory;deep-learning-driven proactive maintenance management;IoT-empowered smart toilet;machine learning algorithms;scheduling maintenance tasks;time-series data","","","","58","IEEE","12 Oct 2022","","","IEEE","IEEE Journals"
"A Hybrid Prediction Method for Realistic Network Traffic With Temporal Convolutional Network and LSTM","J. Bi; X. Zhang; H. Yuan; J. Zhang; M. Zhou","Faculty of Information Technology, School of Software Engineering, Beijing University of Technology, Beijing, China; Faculty of Information Technology, School of Software Engineering, Beijing University of Technology, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; Department of Computer Science, Lyle School of Engineering, Southern Methodist University, Dallas, TX, USA; Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ, USA","IEEE Transactions on Automation Science and Engineering","4 Jul 2022","2022","19","3","1869","1879","Accurate and real-time prediction of network traffic can not only help system operators allocate resources rationally according to their actual business needs but also help them assess the performance of a network and analyze its health status. In recent years, neural networks have been proved suitable to predict time series data, represented by the model of a long short-term memory (LSTM) neural network and a temporal convolutional network (TCN). This article proposes a novel hybrid prediction method named SG and TCN-based LSTM (ST-LSTM) for such network traffic prediction, which synergistically combines the power of the Savitzky–Golay (SG) filter, the TCN, as well as the LSTM. ST-LSTM employs a three-phase end-to-end methodology serving time series prediction. It first eliminates noise in raw data using the SG filter, then extracts short-term features from sequences applying the TCN, and then captures the long-term dependence in the data exploiting the LSTM. Experimental results over real-world datasets demonstrate that the proposed ST-LSTM outperforms state-of-the-art algorithms in terms of prediction accuracy. Note to Practitioners—This work considers real-time and high-accuracy prediction of network traffic. It is highly important to well predict network traffic by capturing long-term dependence and effectively extracting high- and low-frequency information from time series data. Yet, it is a big challenge to achieve it because there are unstable characteristics and strong nonlinear features in the network traffic due to continuous expansion of network scale and fast emergence of new services. Current prediction methods usually have oversimplified theoretical assumptions, need significant time and memory, or suffer problems of gradient disappearance or early convergence. Thus, they fail to effectively capture the nonlinear characteristics of large-scale network sequences. This work proposes a hybrid prediction method named SG and TCN-based LSTM (ST-LSTM), which integrates the merits of the Savitzky–Golay filter, the temporal convolutional network (TCN), and the long short-term memory (LSTM), serving as smoothing time series, capturing short-term local features, and capturing long-term dependence, respectively. Experimental results based on the real-life dataset demonstrate that it achieves better prediction accuracy than its state-of-the-art peers, including the TCN and the LSTM. It can be readily implemented and deployed in many real-life industrial areas including smart city, edge computing, cloud computing, and data centers.","1558-3783","","10.1109/TASE.2021.3077537","Major Science and Technology Program for Water Pollution Control and Treatment of China(grant numbers:2018ZX07111005); National Natural Science Foundation of China (NSFC)(grant numbers:62073005,61802015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9439149","Long short-term memory (LSTM);machine learning;network traffic prediction;Savitzky–Golay (SG) filter;temporal convolutional network (TCN)","Predictive models;Time series analysis;Feature extraction;Deep learning;Load modeling;Task analysis;Support vector machines","cloud computing;computer centres;prediction theory;recurrent neural nets;telecommunication computing;telecommunication traffic;time series","realistic network traffic;temporal convolutional network;time series data;long short-term memory neural network;TCN;hybrid prediction method;SG;Savitzky-Golay filter;ST-LSTM;large-scale network sequences;short-term local features;Savitzky-Golay;cloud computing;edge computing;smart city;data centers","","28","","60","IEEE","21 May 2021","","","IEEE","IEEE Journals"
"6G Connected Vehicle Framework to Support Intelligent Road Maintenance Using Deep Learning Data Fusion","M. Hijji; R. Iqbal; A. Kumar Pandey; F. Doctor; C. Karyotis; W. Rajeh; A. Alshehri; F. Aradah","Faculty of Computers and Information Technology, University of Tabuk, Tabuk, Saudi Arabia; College of Engineering and IT, University of Dubai, Dubai, United Arab Emirates; Interactive Coventry Ltd., Coventry, U.K; Centre for Computational Intelligence, School of Computer Science and Electronic Engineering, University of Essex, Colchester, U.K.; Interactive Coventry Ltd., Coventry, U.K; Faculty of Computers and Information Technology, University of Tabuk, Tabuk, Saudi Arabia; Department of Computer Science, Applied College, University of Tabuk, Tabuk, Saudi Arabia; Centre for Computational Intelligence, School of Computer Science and Electronic Engineering, University of Essex, Colchester, U.K.","IEEE Transactions on Intelligent Transportation Systems","7 Jul 2023","2023","24","7","7726","7735","The growth of IoT, edge and mobile Artificial Intelligence (AI) is supporting urban authorities exploit the wealth of information collected by Connected and Autonomous Vehicles (CAV), to drive the development of transformative intelligent transport applications for addressing smart city challenges. A critical challenge is timely and efficient road infrastructure maintenance. This paper proposes an intelligent hierarchical framework for road infrastructure maintenance that exploits the latest developments in 6G communication technologies, deep learning techniques, and mobile edge AI training approaches. The proposed framework abides with the stringent requirements of training efficient machine learning applications for CAV, and is able to exploit the vast numbers of CAVs forecasted to be present on future road networks. At the core of our framework is a novel Convolution Neural Networks (CNN) model which fuses imagery and sensory data to perform pothole detection. Experiments show the proposed model can achieve state of the art performance in comparison to existing approaches while being simple, cost-effective and computationally efficient to deploy. The proposed system can form part of a federated learning framework for facilitating large scale real-time road surface condition monitoring and support adaptive resource allocation for road infrastructure maintenance.","1558-0016","","10.1109/TITS.2023.3235151","Deanship of Scientific Research at the University of Tabuk(grant numbers:0195-1442-S); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10021225","6G;deep learning;mobile edge intelligence;pothole detection;federated learning;intelligent transportation systems","Roads;Maintenance engineering;Data models;Computational modeling;6G mobile communication;Training;Smart phones","5G mobile communication;6G mobile communication;artificial intelligence;condition monitoring;convolutional neural nets;deep learning (artificial intelligence);intelligent transportation systems;Internet of Things;learning (artificial intelligence);neural nets;object detection;resource allocation;road vehicles;roads;sensor fusion;smart cities;traffic engineering computing","6G communication technologies;6G Connected vehicle framework;CAV;deep learning data fusion;deep learning techniques;efficient road infrastructure maintenance;federated learning framework;framework abides;future road networks;intelligent hierarchical framework;intelligent road maintenance;mobile edge AI training approaches;novel Convolution Neural Networks model;scale real-time road surface condition monitoring;sensory data;smart city challenges;support adaptive resource allocation;training efficient machine;transformative intelligent transport applications;urban authorities","","3","","60","IEEE","18 Jan 2023","","","IEEE","IEEE Journals"
"ST-Bikes: Predicting Travel-Behaviors of Sharing-Bikes Exploiting Urban Big Data","J. Chai; J. Song; H. Fan; Y. Xu; L. Zhang; B. Guo; Y. Xu","College of Computer Science, Sichuan University, Chengdu, China; Department of Civil and Environment Engineering, Imperial College London, London, U.K.; Department of Natural Sciences, Imperial College London, London, U.K.; SULON AI Laboratory, Nanjing, China; College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; H. K. AI Information Laboratory, Hong Kong Science Park, Hong Kong","IEEE Transactions on Intelligent Transportation Systems","7 Jul 2023","2023","24","7","7676","7686","With the development of the modern smart city, sharing-bikes require behaviors prediction for grid-level areas which is essential for intelligent transportation systems. A model which can predict bike sharing demand behaviours accurately can allocate sharing-bikes in advance to satisfy travel demands alongside saving energy, reducing traffic, cutting down waste for those sharing-bikes companies putting excessive sharing-bikes in unsaturated demand areas. In this paper, we abandon the traditional time series prediction method and use a more efficient deep learning method to solve the traffic forecasting problem. Moreover, instead of considering spatial relation and temporal relation relatively, we produced a deep multi-view spatial-temporal network to combine them into one prediction model framework. In the experimental section, we investigate in the experiment on enormous amount of real sharing-bikes application use data in the core region of Beijing to test the performance of the model framework with a 1 km  $\times $  1 km grid-level scale and compare it with other existing machine learning approaches and prediction models. And the 4G/5G/6G communication technology facilitate the real-time control of the space-time locations of sharing bikes dynamically. Thus, it provides the basis for high-frequency analysis of space-time patterns, especially supported by the 6G large-scale application in the future.","1558-0016","","10.1109/TITS.2022.3197778","National Natural Science Foundation of China(grant numbers:62172061); National Key Research and Development Project(grant numbers:2020YFB1711800,2020YFB1707900); Sichuan Province Science and Technology Support Program(grant numbers:2021-YFG0152,2021YFG0025,2020YFG0479,2020YFG0322,2020GFW035,2020GFW033); Research and Development Project of Chengdu City(grant numbers:2019-YF05-01790-GX); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9861705","Sharing-bikes prediction;ITS;deep learning;multi-view;spatial-temporal feature;travel-behaviors 4G/5G/6G communication","Predictive models;Data models;Roads;Urban areas;Time series analysis;Meteorology;Public transportation","5G mobile communication;6G mobile communication;bicycles;Big Data;deep learning (artificial intelligence);intelligent transportation systems;road traffic;smart cities;traffic engineering computing;transportation","4G communication;5G communication;6G communication;deep learning;deep multiview spatial-temporal network;grid-level areas;high-frequency analysis;intelligent transportation systems;prediction model framework;sharing-bikes application;sharing-bikes companies;size 1.0 km;smart city;space-time patterns;spatial relation;ST-bikes;temporal relation;traffic forecasting problem;travel demands;travel-behaviors prediction;unsaturated demand areas;urban Big Data","","","","57","IEEE","18 Aug 2022","","","IEEE","IEEE Journals"
"Digital Twins From Smart Manufacturing to Smart Cities: A Survey","G. Mylonas; A. Kalogeras; G. Kalogeras; C. Anagnostopoulos; C. Alexakos; L. Muñoz","Industrial Systems Institute, Athena Research and Innovation Center, Patras, Greece; Industrial Systems Institute, Athena Research and Innovation Center, Patras, Greece; Industrial Systems Institute, Athena Research and Innovation Center, Patras, Greece; Industrial Systems Institute, Athena Research and Innovation Center, Patras, Greece; Industrial Systems Institute, Athena Research and Innovation Center, Patras, Greece; Department of Communications Engineering, University of Cantabria, Santander, Spain","IEEE Access","27 Oct 2021","2021","9","","143222","143249","Digital twins are quickly becoming a popular tool in several domains, taking advantage of recent advancements in the Internet of Things, Machine Learning and Big Data, while being used by both the industry sector and the research community. In this paper, we review the current research landscape as regards digital twins in the field of smart cities, while also attempting to draw parallels with the application of digital twins in Industry 4.0. Although digital twins have received considerable attention in the Industrial Internet of Things domain, their utilization in smart cities has not been as popular thus far. We discuss here the open challenges in the field and argue that digital twins in smart cities should be treated differently and be considered as cyber-physical “systems of systems”, due to the vastly different system size, complexity and requirements, when compared to other recent applications of digital twins. We also argue that researchers should utilize established tools and methods of the smart city community, such as co-creation, to better handle the specificities of this domain in practice.","2169-3536","","10.1109/ACCESS.2021.3120843","Project “I3T—Innovative Application of Industrial Internet of Things (IIoT) in Smart Environments” (MIS 5002434) implemented under the “Action for the Strategic Development on the Research and Technological Sector,”; Operational Programme “Competitiveness, Entrepreneurship and Innovation”(grant numbers:NSRF 2014–2020); Greece and the European Union (European Regional Development Fund); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9576739","Digital twin;smart cities;industry 4.0;society 5.0;IoT;smart manufacturing;cyber-physical systems;open challenges","Digital twin;Smart cities;Smart manufacturing;Tools;Technological innovation;Real-time systems;Production","Big Data;cyber-physical systems;Internet of Things;learning (artificial intelligence);production engineering computing;smart cities","smart cities;digital twins;smart manufacturing;Internet of Things;machine learning;Big Data;Industry 4.0;cyber-physical","","57","","147","CCBY","15 Oct 2021","","","IEEE","IEEE Journals"
"AttentionCode: Ultra-Reliable Feedback Codes for Short-Packet Communications","Y. Shao; E. Ozfatura; A. G. Perotti; B. M. Popović; D. Gündüz","State Key Laboratory of Internet of Things for Smart City, University of Macau, Macau, Taipa, China; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.; Radio Transmission Technology Laboratory, Huawei Technologies Sweden AB, Kista, Sweden; Radio Transmission Technology Laboratory, Huawei Technologies Sweden AB, Kista, Sweden; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.","IEEE Transactions on Communications","15 Aug 2023","2023","71","8","4437","4452","Ultra-reliable short-packet communication is a major challenge in future wireless networks with critical applications. To achieve ultra-reliable communications beyond 99.999%, this paper envisions a new interaction-based communication paradigm that exploits feedback from the receiver. We present AttentionCode, a new class of feedback codes leveraging deep learning (DL) technologies. The underpinnings of AttentionCode are three architectural innovations: AttentionNet, input restructuring, and adaptation to fading channels, accompanied by several training methods, including large-batch training, distributed learning, look-ahead optimizer, training-test signal-to-noise ratio (SNR) mismatch, and curriculum learning. The training methods can potentially be generalized to other wireless communication applications with machine learning. Numerical experiments verify that AttentionCode establishes a new state of the art among all DL-based feedback codes in both additive white Gaussian noise (AWGN) channels and fading channels. In AWGN channels with noiseless feedback, for example, AttentionCode achieves a block error rate (BLER) of 10−7 when the forward channel SNR is 0 dB for a block size of 50 bits, demonstrating the potential of AttentionCode to provide ultra-reliable short-packet communications.","1558-0857","","10.1109/TCOMM.2023.3280563","UKRI for the projects AIR (ERC-CoG, EP/X030806/1) and SONATA (EPSRCEP/W035960/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10138113","Ultra-reliable short-packet communications;feedback;deep learning;the attention mechanism","Codes;Signal to noise ratio;Receivers;Training;Noise measurement;Fading channels;Feedforward systems","","","","","","38","IEEE","29 May 2023","","","IEEE","IEEE Journals"
"Temporal Weighting Appearance-Aligned Network for Nighttime Video Retrieval","W. Ruan; Y. Tao; L. Ruan; X. Shu; Y. Qiao","Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; School of Foreign Languages and Literature, Wuhan University, Wuhan, China; Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, Chongqing, China; Tencent Toutu Lab, Shenzhen, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China","IEEE Signal Processing Letters","29 Sep 2022","2022","29","","2008","2012","Video-based person re-identification (ReID) aims at re-identifying video sequences of a specified person from videos captured by disjoint cameras. Existing datasets and works on this task all focus on daytime scenarios and cannot adapt well to the nighttime scenarios, which is also of significant importance for practical applications. In this letter, we contribute a new dataset for nighttime video-based ReID, termed NIVIR, which contains 800 identities with over 228,000 images. NIVIR contains video shots under various lighting conditions, different weathers, and complex scenarios, which is consistent with the real nighttime outdoor surveillance. Furthermore, we propose a temporal weighting appearance-aligned network (TWAN) for nighttime video-based ReID, which is composed of a correlation-based appearance-aligned module (CAM) and a temporal weighting module (TWM). Specifically, CAM is proposed to reconstruct the adjacent feature maps to guarantee the appearance alignment between the central frame and its adjacent frames. TWM is designed to evaluate the frame quality of a tracklet and generate temporal weights to enhance the video representation. Extensive experiments conducted on our new NIVIR dataset demonstrate that the proposed TWAN outperforms the state-of-the-art methods. We believe that our NIVIR dataset and the comprehensive attempts for solving the nighttime ReID problem will push forward the development of the ReID research community.","1558-2361","","10.1109/LSP.2022.3207620","China Postdoctoral Science Foundation(grant numbers:2021M693288); National Nature Science Foundations of China(grant numbers:U20B2052); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9894684","Nighttime video retrieval;new benchmark;appearance alignment;temporal weighting","Task analysis;Cameras;Feature extraction;Convolution;Surveillance;Three-dimensional displays;Meteorology","cameras;feature extraction;image representation;image sequences;object detection;object tracking;video retrieval;video signal processing;video surveillance","temporal weighting appearance-aligned network;nighttime video retrieval;video-based person re-identification;video sequences;daytime scenarios;nighttime scenarios;nighttime video-based ReID;video shots;nighttime outdoor surveillance;correlation-based appearance-aligned module;temporal weighting module;appearance alignment;temporal weights;video representation;NIVIR dataset;nighttime ReID problem","","","","35","IEEE","19 Sep 2022","","","IEEE","IEEE Journals"
"Denoising Noisy Neural Networks: A Bayesian Approach With Compensation","Y. Shao; S. C. Liew; D. Gündüz","State Key Laboratory of Internet of Things for Smart City, University of Macau, Macau, China; Department of Information Engineering, The Chinese University of Hong Kong, Shatin, Hong Kong; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.","IEEE Transactions on Signal Processing","13 Jul 2023","2023","71","","2460","2474","Deep neural networks (DNNs) with noisy weights, which we refer to as noisy neural networks (NoisyNNs), arise from the training and inference of DNNs in the presence of noise. NoisyNNs emerge in many new applications, including the wireless transmission of DNNs, the efficient deployment or storage of DNNs in analog devices, and the truncation or quantization of DNN weights. This article studies a fundamental problem of NoisyNNs: how to reconstruct the DNN weights from their noisy manifestations. While prior works relied exclusively on the maximum likelihood (ML) estimation, this article puts forth a denoising approach to reconstruct DNNs with the aim of maximizing the inference accuracy of the reconstructed models. The superiority of our denoiser is rigorously proven in two small-scale problems, wherein we consider a quadratic neural network function and a shallow feedforward neural network, respectively. When applied to advanced learning tasks with modern DNN architectures, our denoiser exhibits significantly better performance than the ML estimator. Consider the average test accuracy of the denoised DNN model versus the weight variance to noise power ratio (WNR) performance. When denoising a noisy ResNet34 model arising from noisy inference, our denoiser outperforms ML estimation by up to 4.1 dB to achieve a test accuracy of 60%. When denoising a noisy ResNet18 model arising from noisy training, our denoiser outperforms ML estimation by 13.4 dB and 8.3 dB to achieve test accuracies of 60% and 80%, respectively.","1941-0476","","10.1109/TSP.2023.3290327","U.K. Research and Innovation; AIR(grant numbers:ERC-CoG,EP/X030806/1,SONATA EPSRC-EP/W035960/1); CUHK Direct(grant numbers:4055157); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10167840","Noisy neural network;denoiser;wireless transmission of neural networks;federated edge learning","Noise measurement;Training;Maximum likelihood estimation;Wireless communication;Neural networks;Estimation;Noise reduction","Bayes methods;deep learning (artificial intelligence);feedforward neural nets;maximum likelihood estimation;neural net architecture;signal denoising","Bayesian approach;deep neural networks;denoiser;denoising approach;DNN architectures;DNN weights;maximum likelihood estimation;ML estimation;noise power ratio performance;noisy neural networks;noisy weights;NoisyNNs;quadratic neural network function;ResNet18 model;ResNet34 model;shallow feedforward neural network","","","","49","IEEE","28 Jun 2023","","","IEEE","IEEE Journals"
"Short-Term Prediction of City Traffic Flow via Convolutional Deep Learning","S. Bilotta; E. Collini; P. Nesi; G. Pantaleo","Department of Information Engineering, Distributed Systems and Internet Technologies Laboratory, University of Florence, Florence, Italy; Department of Information Engineering, Distributed Systems and Internet Technologies Laboratory, University of Florence, Florence, Italy; Department of Information Engineering, Distributed Systems and Internet Technologies Laboratory, University of Florence, Florence, Italy; Department of Information Engineering, Distributed Systems and Internet Technologies Laboratory, University of Florence, Florence, Italy","IEEE Access","2 Nov 2022","2022","10","","113086","113099","Nowadays, traffic management and sustainable mobility are central topics for intelligent transportation systems (ITS). Thanks to new technologies, it is possible to collect real-time data to monitor the traffic situation and contextual information by sensors. An important challenge in ITS is the ability to predict road traffic flow data. The short-term predictions (10-60 minutes) of traffic flow data is a complex nonlinear task that has been the subject of many research efforts in past few decades. Accessing traffic flow data is mandatory for a large number of applications that have to guarantee a high level of services such as traffic flow analysis, traffic flow reconstruction, which in their turn are used to compute predictions needed to perform what-if analysis, forecast routing, conditioned routing, predictions of pollutant, etc. This paper proposes a solution for short-term prediction of traffic flow data by using a architecture capable to exploit Convolutional Bidirectional Deep Long Short Term Memory neural networks (CONV-BI-LSTM). The solution adopts a different architecture and features, so as to overcome the state-of-the-art solutions and provides precise predictions addressing traffic flow data in cities, which are tendentially very noisy with respect to the ones measured in high-speed roads, the latter being the validation context for the majority of state-of-the-art solutions. The proposed solution has been developed and validated in the city context and data via Sii-Mobility, a smart city mobility and transport national project and it is currently in use in other contexts such as in Snap4City PCP EC, TRAFAIR CEF, and REPLICATE H2020 SCC1, and it is operative in those areas.","2169-3536","","10.1109/ACCESS.2022.3217240","Ministero dell?Istruzione, dell?Universit? e della Ricerca(grant numbers:SC112 Sii-Mobility); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9930774","Traffic flow;short-term predictions;machine learning;deep learning;CONV-BI-LSTM","Sensors;Road traffic;Predictive models;Deep learning;Neural networks;Real-time systems;Pollution measurement;Traffic control","convolutional neural nets;deep learning (artificial intelligence);intelligent transportation systems;recurrent neural nets;road traffic;traffic engineering computing;transportation","short-term prediction;city traffic flow;traffic management;road traffic flow data;traffic flow reconstruction;sustainable mobility;intelligent transportation systems;ITS;complex nonlinear task;CONV-BI-LSTM;convolutional bidirectional deep long short term memory neural networks;transport national project","","1","","64","CCBYNCND","26 Oct 2022","","","IEEE","IEEE Journals"
"Series Editorial: Design and Implementation of Devices, Circuits, and Systems","B. Fong; H. Kim; V. Sai","Chair of the System Biology and Biomedical Systems Technical Committee, IEEE Systems Council, USA; 5G and beyond network team, VTT Technical Research Centre of Finland, Finland; Visual and Machine Learning IP Team, Intel, CA, USA","IEEE Communications Magazine","17 May 2021","2021","59","4","124","124","The Design and Implementation of Devices, Circuits, and Systems Series attracts original articles that cover different application areas in technological advances and developments of communication devices, circuits and systems. The Series welcomes contributions from various industrial sectors such as healthcare, automotive, energy, agriculture, smart manufacturing, consumer electronics, smart city, VR/AR/ hologram, drone and consumer electronics.","1558-1896","","10.1109/MCOM.2021.9433523","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9433523","","Special sections and issues;Smart devices;Communications technology;Consumer electronics","","","","","","0","IEEE","17 May 2021","","","IEEE","IEEE Magazines"
"AI’s 10 to Watch, 2022","J. Dix; Z. Zhang","Clausthal University of Technology, Clausthal, Germany; Binghamton University, State University of New York, Binghamton, NY, USA","IEEE Intelligent Systems","28 Apr 2023","2023","38","2","3","14","IEEE Intelligent Systems is promoting young and aspiring artificial intelligence (AI) scientists and recognizing the rising stars as “AI‘s 10 Watch.” This biennial 2022 edition is slightly different from the previous editions: We solicited submissions from individuals who had obtained their Ph.D. up to 10 years prior (as opposed to 5 years in all of the previous editions). This led to more applications of the highest quality. The selection committee finally had to select 10 outstanding contributors from a pool of 30+ highly competitive and strong nominations, which made the selection decisions rather difficult. After a careful and detailed selection process through many rounds of discussions via e-mails and live meetings, the committee voted unanimously on a short list of 10 top candidates who have all demonstrated outstanding achievements in different areas of AI. The selection was based solely on scientific quality, reputation, impact, and expert endorsements accumulated since their Ph.D. It is our honor and privilege to announce the following 2022 class of “AI’s 10 to Watch.”• Bo Li. She is working on trustworthy machine learning (ML) at the intersection of ML, security and privacy, and game theory. She was able to integrate domain knowledge and logical reasoning abilities into data-driven statistical ML models to improve learning robustness with guarantees, and she has designed scalable privacy-preserving data-publishing frameworks for high-dimensional data. Her work has provided rigorous guarantees for the trustworthiness of learning systems and been deployed in industrial applications. She is an assistant professor with the University of Illinois at Urbana-Champaign.• Tongliang Liu. He is working in the fields of trustworthy ML. His work in theories and algorithms of ML with noisy labels has led to significant contributions and influence in the fields of ML, computer vision, natural language processing (NLP), and data mining, as large-scale datasets in those fields are prone to suffering severe label errors. He is a senior lecturer at the School of Computer Science, University of Sydney, and a visiting associate professor at the Department of Machine Learning, Mohamed bin Zayed University of Artificial Intelligence.• Liqiang Nie. He is the dean of and a professor with the School of Computer Science and Technology, Harbin Institute of Technology (Shenzhen). He works on multimedia content analysis and search, with a particular emphasis on data-driven multimodal learning and knowledge-guided multimodal reasoning. He pioneered the explicit modeling of consistent, complementary, and partial alignment relationships among modalities.• Soujanya Poria. He is an assistant professor at Singapore University of Technology and Design (SUTD). His seminal research on fusing information from textual, audio, and visual modalities for diverse behavioral and affective tasks significantly improved systems reliant on multimodal data, paving the way to various novel research avenues. His latest works are on information extraction, vision–language reasoning, and understanding human conversations in terms of common sense-based, context-grounded causal explanations.• Deqing Sun. He is a staff research scientist at Google. He has made significant contributions to computer vision, in particular in motion estimation. His work on optical flow (“Classic+NL” and “PWC-Net”) has been very influential and has been powering commercial applications such as Super SloMo in NVIDIA’s RTX platform, Face Unblur, and Fusion Zoom on Google’s Pixel phone.• Yizhou Sun. She is a pioneer in heterogeneous information network (HIN) mining, with a recent focus on deep graph learning, neural symbolic reasoning, and providing neural solutions to multiagent dynamical systems. Her work has a wide spectrum of applications, ranging from e-commerce, health care, and material science to hardware design. She is currently an associate professor at the University of California, Los Angeles (UCLA).• Jiliang Tang. He is a University Foundation Professor at Michigan State University. He works on graph ML and trustworthy AI and their applications in education and biology. His contributions to these fields include highly cited algorithms, well-received systems, and popular books.• Zhangyang “Atlas” Wang. He works on efficient and reliable ML. Recently, his core research theme is to leverage, understand, and expand the role of sparsity, from classical optimization to modern neural networks (NNs), whose impacts span the efficient training/inference of large-foundation models, robustness and trustworthiness, generative AI, graph learning, and more.• Hongzhi Yin. He has worked on trustworthy data intelligence to turn data into privacy-preserving, robust, explainable, and fair intelligent services in various industries and scenarios. He is also a leading expert researching and developing next-generation intelligent systems and algorithms for lightweight on-device predictive analytics as well as recommendation and decentralized ML on massive and heterogeneous data. He is an associate professor and ARC Future Fellow at the University of Queensland.• Liang Zheng. He is a senior lecturer at the Australian National University and works on data-centric computer vision, where he seeks to improve the quality of training and validation data, predict test data difficulty without labels, and more. These efforts provide a complementary perspective to model-centric developments. He has also made significant contributions to object re-identification and the broader smart city initiative through the introduction of widely used benchmarks and baseline methods.","1941-1294","","10.1109/MIS.2023.3252919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10111517","","Intelligent systems;Artificial intelligence;Companies;Computer applications","","","","","","0","IEEE","28 Apr 2023","","","IEEE","IEEE Magazines"
